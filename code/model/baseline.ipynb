{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb5d5dd5",
        "outputId": "7c1e8f39-edb5-4848-f327-821aac0193e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# github_recsys_twotower_with_robust_eval.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Optional: for AUC metric\n",
        "try:\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    SKLEARN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SKLEARN_AVAILABLE = False\n",
        "\n",
        "\n",
        "########################################\n",
        "# 1. CONFIG AND FEATURE DEFINITIONS\n",
        "########################################\n",
        "\n",
        "class Config:\n",
        "    # File paths: adapt these to your actual locations\n",
        "    train_balanced_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/train_balanced.csv\"\n",
        "    train_negative_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/train_negative.csv\"\n",
        "    test_balanced_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/test_balanced.csv\"\n",
        "    test_negative_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/test_negative.csv\"\n",
        "\n",
        "    # Model hyperparameters\n",
        "    user_id_emb_dim = 64\n",
        "    item_id_emb_dim = 64\n",
        "    lang_emb_dim = 16\n",
        "    hidden_dim = 128\n",
        "    embedding_dim = 64  # final tower output dimension\n",
        "\n",
        "    batch_size = 2048\n",
        "    num_epochs = 10\n",
        "    lr = 1e-3\n",
        "    weight_decay = 1e-5\n",
        "\n",
        "    # Contrastive Loss Config\n",
        "    temperature = 0.1  # Scale factor for cosine similarity logits\n",
        "\n",
        "    # Validation split\n",
        "    train_user_fraction = 0.8  # 80% users for train, 20% for validation\n",
        "\n",
        "    # Ranking evaluation config\n",
        "    eval_k_list = [5, 10]\n",
        "    eval_num_negatives = 100  # per user for ranking eval\n",
        "\n",
        "    # CUDA if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Random seeds for reproducibility\n",
        "    seed = 42\n",
        "\n",
        "\n",
        "# Columns in csvs\n",
        "NUMERIC_REPO_COLS = [\n",
        "    \"watchers\", \"commits\", \"issues\", \"pull_requests\",\n",
        "    \"mean_commits_language\", \"max_commits_language\", \"min_commits_language\",\n",
        "    \"std_commits_language\",\n",
        "    \"mean_pull_requests_language\", \"max_pull_requests_language\",\n",
        "    \"min_pull_requests_language\", \"std_pull_requests_language\",\n",
        "    \"mean_issues_language\", \"max_issues_language\", \"min_issues_language\",\n",
        "    \"std_issues_language\",\n",
        "    \"mean_watchers_language\", \"max_watchers_language\",\n",
        "    \"min_watchers_language\", \"std_watchers_language\",\n",
        "    \"events\", \"year\",\n",
        "    \"weight\", \"cp\", \"avg_cp\", \"stddev\",\n",
        "]\n",
        "\n",
        "CATEGORICAL_REPO_COLS = [\n",
        "    \"language_code\",\n",
        "]\n",
        "\n",
        "USER_ID_COL = \"id_user\"\n",
        "ITEM_ID_COL = \"project_id\"\n",
        "TARGET_COL = \"target\""
      ],
      "metadata": {
        "id": "9Et7DSEWD5Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 2. DATA LOADING AND PREPROCESSING\n",
        "########################################\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def load_raw_data(cfg: Config):\n",
        "    train_bal = pd.read_csv(cfg.train_balanced_path)\n",
        "    train_neg = pd.read_csv(cfg.train_negative_path)\n",
        "    test_bal = pd.read_csv(cfg.test_balanced_path)\n",
        "    test_neg = pd.read_csv(cfg.test_negative_path)\n",
        "    return train_bal, train_neg, test_bal, test_neg\n",
        "\n",
        "\n",
        "def build_id_mappings(train_pool: pd.DataFrame, test_df: pd.DataFrame):\n",
        "    # Unique users and items across train_pool + test\n",
        "    all_users = pd.concat([train_pool[USER_ID_COL], test_df[USER_ID_COL]]).unique()\n",
        "    all_items = pd.concat([train_pool[ITEM_ID_COL], test_df[ITEM_ID_COL]]).unique()\n",
        "    all_langs = pd.concat([train_pool[\"language_code\"], test_df[\"language_code\"]]).unique()\n",
        "\n",
        "    user2idx = {uid: i for i, uid in enumerate(all_users)}\n",
        "    item2idx = {iid: i for i, iid in enumerate(all_items)}\n",
        "    lang2idx = {lid: i for i, lid in enumerate(all_langs)}\n",
        "\n",
        "    idx2user = {i: uid for uid, i in user2idx.items()}\n",
        "    idx2item = {i: iid for iid, i in item2idx.items()}\n",
        "    idx2lang = {i: lid for lid, i in lang2idx.items()}\n",
        "\n",
        "    return user2idx, item2idx, lang2idx, idx2user, idx2item, idx2lang\n",
        "\n",
        "\n",
        "def compute_numeric_scalers(train_pool: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute mean and std for each numeric repo feature on training pool data.\n",
        "    Returns dicts: col -> (mean, std).\n",
        "    \"\"\"\n",
        "    means = {}\n",
        "    stds = {}\n",
        "    for col in NUMERIC_REPO_COLS:\n",
        "        col_values = train_pool[col].astype(float).values\n",
        "        mean = col_values.mean()\n",
        "        std = col_values.std()\n",
        "        if std < 1e-6:\n",
        "            std = 1.0\n",
        "        means[col] = mean\n",
        "        stds[col] = std\n",
        "    return means, stds\n",
        "\n",
        "\n",
        "def normalize_numeric_features(df: pd.DataFrame, means: Dict[str, float], stds: Dict[str, float]):\n",
        "    df = df.copy()\n",
        "    for col in NUMERIC_REPO_COLS:\n",
        "        df[col] = (df[col].astype(float) - means[col]) / stds[col]\n",
        "    return df\n",
        "\n",
        "\n",
        "def split_train_val_by_user(\n",
        "    train_pool: pd.DataFrame,\n",
        "    train_user_fraction: float,\n",
        "    seed: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Split the training pool into train_df and val_df based on users.\n",
        "    \"\"\"\n",
        "    all_users = train_pool[USER_ID_COL].unique()\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(all_users)\n",
        "\n",
        "    n_train_users = int(len(all_users) * train_user_fraction)\n",
        "    train_users = set(all_users[:n_train_users])\n",
        "    val_users = set(all_users[n_train_users:])\n",
        "\n",
        "    train_df = train_pool[train_pool[USER_ID_COL].isin(train_users)].reset_index(drop=True)\n",
        "    val_df = train_pool[train_pool[USER_ID_COL].isin(val_users)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def build_item_feature_table_norm(\n",
        "    train_pool: pd.DataFrame,\n",
        "    test_df: pd.DataFrame,\n",
        "    means: Dict[str, float],\n",
        "    stds: Dict[str, float],\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a unique, normalized item feature table: one row per project_id.\n",
        "    \"\"\"\n",
        "    all_df = pd.concat([train_pool, test_df], ignore_index=True)\n",
        "    all_df_norm = normalize_numeric_features(all_df, means, stds)\n",
        "\n",
        "    # Keep one row per item; using last occurrence (could also aggregate)\n",
        "    all_df_norm = all_df_norm.sort_values(\"events\")\n",
        "    item_table = all_df_norm.drop_duplicates(subset=[ITEM_ID_COL], keep=\"last\")\n",
        "    item_table = item_table.reset_index(drop=True)\n",
        "    return item_table"
      ],
      "metadata": {
        "id": "t0AQ8oxwFsWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 3. DATASET AND DATALOADER\n",
        "########################################\n",
        "\n",
        "class TwoTowerDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        user2idx: Dict[int, int],\n",
        "        item2idx: Dict[int, int],\n",
        "        lang2idx: Dict[int, int],\n",
        "    ):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.user2idx = user2idx\n",
        "        self.item2idx = item2idx\n",
        "        self.lang2idx = lang2idx\n",
        "\n",
        "        # Pre-extract numpy arrays so __getitem__ is fast\n",
        "        self.user_ids = self.df[USER_ID_COL].values\n",
        "        self.item_ids = self.df[ITEM_ID_COL].values\n",
        "        self.lang_codes = self.df[\"language_code\"].values\n",
        "        self.labels = self.df[TARGET_COL].astype(float).values\n",
        "\n",
        "        self.numeric_matrix = self.df[NUMERIC_REPO_COLS].astype(float).values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        uid = self.user_ids[idx]\n",
        "        iid = self.item_ids[idx]\n",
        "        lang = self.lang_codes[idx]\n",
        "        label = self.labels[idx]\n",
        "        numerics = self.numeric_matrix[idx]\n",
        "\n",
        "        u_idx = self.user2idx[uid]\n",
        "        i_idx = self.item2idx[iid]\n",
        "        l_idx = self.lang2idx[lang]\n",
        "\n",
        "        # Convert to tensors\n",
        "        u_idx = torch.tensor(u_idx, dtype=torch.long)\n",
        "        i_idx = torch.tensor(i_idx, dtype=torch.long)\n",
        "        l_idx = torch.tensor(l_idx, dtype=torch.long)\n",
        "        numerics = torch.tensor(numerics, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        return u_idx, i_idx, l_idx, numerics, label\n",
        "\n",
        "\n",
        "def make_dataloaders(\n",
        "    cfg: Config,\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    user2idx: Dict[int, int],\n",
        "    item2idx: Dict[int, int],\n",
        "    lang2idx: Dict[int, int],\n",
        "):\n",
        "    train_dataset = TwoTowerDataset(train_df, user2idx, item2idx, lang2idx)\n",
        "    val_dataset = TwoTowerDataset(val_df, user2idx, item2idx, lang2idx)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "FJP3C0LfFyMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 4. MODEL DEFINITIONS\n",
        "########################################\n",
        "\n",
        "class UserTower(nn.Module):\n",
        "    def __init__(self, num_users: int, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(num_users, cfg.user_id_emb_dim)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.user_id_emb_dim, cfg.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(cfg.hidden_dim, cfg.embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, user_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        user_ids: [B]\n",
        "        returns: [B, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = self.user_emb(user_ids)  # [B, user_id_emb_dim]\n",
        "        x = self.mlp(x)              # [B, embedding_dim]\n",
        "        x = nn.functional.normalize(x, p=2, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ItemTower(nn.Module):\n",
        "    def __init__(self, num_items: int, num_langs: int, num_numeric_feats: int, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.item_emb = nn.Embedding(num_items, cfg.item_id_emb_dim)\n",
        "        self.lang_emb = nn.Embedding(num_langs, cfg.lang_emb_dim)\n",
        "\n",
        "        input_dim = cfg.item_id_emb_dim + cfg.lang_emb_dim + num_numeric_feats\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, cfg.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(cfg.hidden_dim, cfg.embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        item_ids: torch.Tensor,     # [B]\n",
        "        lang_ids: torch.Tensor,     # [B]\n",
        "        numeric_feats: torch.Tensor # [B, num_numeric_feats]\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        returns: [B, embedding_dim]\n",
        "        \"\"\"\n",
        "        item_e = self.item_emb(item_ids)    # [B, item_id_emb_dim]\n",
        "        lang_e = self.lang_emb(lang_ids)    # [B, lang_emb_dim]\n",
        "\n",
        "        x = torch.cat([item_e, lang_e, numeric_feats], dim=-1)  # [B, input_dim]\n",
        "        x = self.mlp(x)                                         # [B, embedding_dim]\n",
        "        x = nn.functional.normalize(x, p=2, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TwoTowerRecSys(nn.Module):\n",
        "    def __init__(self, num_users: int, num_items: int, num_langs: int, num_numeric_feats: int, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.user_tower = UserTower(num_users, cfg)\n",
        "        self.item_tower = ItemTower(num_items, num_langs, num_numeric_feats, cfg)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        user_ids: torch.Tensor,\n",
        "        item_ids: torch.Tensor,\n",
        "        lang_ids: torch.Tensor,\n",
        "        numeric_feats: torch.Tensor,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        user_ids: [B]\n",
        "        item_ids: [B]\n",
        "        lang_ids: [B]\n",
        "        numeric_feats: [B, F]\n",
        "\n",
        "        returns:\n",
        "            logits: [B] (dot products)\n",
        "            user_embs: [B, D]\n",
        "            item_embs: [B, D]\n",
        "        \"\"\"\n",
        "        u = self.user_tower(user_ids)                          # [B, D]\n",
        "        v = self.item_tower(item_ids, lang_ids, numeric_feats) # [B, D]\n",
        "\n",
        "        logits = torch.sum(u * v, dim=-1)  # [B]\n",
        "        return logits, u, v"
      ],
      "metadata": {
        "id": "gFhKqCa3F22R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(\n",
        "    model: TwoTowerRecSys,\n",
        "    train_loader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: str,\n",
        "    temperature: float = 0.1,\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    # Use CrossEntropyLoss for In-Batch Negatives (Contrastive Learning)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # Unpack batch\n",
        "        # Note: In this mode, we expect 'batch' to contain only POSITIVE pairs.\n",
        "        # 'labels' from the dataset are ignored because the targets are implicit (the diagonal).\n",
        "        user_ids, item_ids, lang_ids, numerics, _ = batch\n",
        "\n",
        "        user_ids = user_ids.to(device)\n",
        "        item_ids = item_ids.to(device)\n",
        "        lang_ids = lang_ids.to(device)\n",
        "        numerics = numerics.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get embeddings from the model\n",
        "        # forward() returns: logits, user_embs, item_embs\n",
        "        # We ignore the pointwise 'logits' and use the embeddings directly.\n",
        "        _, u_emb, v_emb = model(user_ids, item_ids, lang_ids, numerics)\n",
        "\n",
        "        # Compute Similarity Matrix: [Batch_Size, Batch_Size]\n",
        "        # Both u_emb and v_emb are L2 normalized in the model, so this is Cosine Similarity.\n",
        "        sim_matrix = torch.matmul(u_emb, v_emb.T)\n",
        "\n",
        "        # Scale by temperature to sharpen the distribution\n",
        "        logits = sim_matrix / temperature\n",
        "\n",
        "        # The target for user i is item i (the diagonal)\n",
        "        batch_size = user_ids.size(0)\n",
        "        targets = torch.arange(batch_size, device=device, dtype=torch.long)\n",
        "\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_examples += batch_size\n",
        "\n",
        "    avg_loss = total_loss / max(total_examples, 1)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def evaluate_pointwise(\n",
        "    model: TwoTowerRecSys,\n",
        "    data_loader: DataLoader,\n",
        "    device: str,\n",
        "):\n",
        "    \"\"\"\n",
        "    Simple pointwise evaluation:\n",
        "    - average loss\n",
        "    - ROC-AUC (if sklearn available)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            user_ids, item_ids, lang_ids, numerics, labels = batch\n",
        "            user_ids = user_ids.to(device)\n",
        "            item_ids = item_ids.to(device)\n",
        "            lang_ids = lang_ids.to(device)\n",
        "            numerics = numerics.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits, _, _ = model(user_ids, item_ids, lang_ids, numerics)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "\n",
        "            batch_size = labels.size(0)\n",
        "            total_loss += loss.item() * batch_size\n",
        "            total_examples += batch_size\n",
        "\n",
        "            all_labels.append(labels.detach().cpu().numpy())\n",
        "            all_probs.append(probs.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / max(total_examples, 1)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_probs = np.concatenate(all_probs, axis=0)\n",
        "\n",
        "    if SKLEARN_AVAILABLE:\n",
        "        try:\n",
        "            auc = roc_auc_score(all_labels, all_probs)\n",
        "        except ValueError:\n",
        "            auc = float(\"nan\")\n",
        "    else:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    return avg_loss, auc"
      ],
      "metadata": {
        "id": "n9ErgLejGBNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 6. ROBUST RANKING METRICS (Recall@K, NDCG@K)\n",
        "########################################\n",
        "\n",
        "def compute_dcg(relevances: np.ndarray, k: int) -> float:\n",
        "    \"\"\"\n",
        "    relevances: binary or graded relevance scores sorted in ranking order.\n",
        "    \"\"\"\n",
        "    rel = relevances[:k]\n",
        "    if len(rel) == 0:\n",
        "        return 0.0\n",
        "    discounts = np.log2(np.arange(2, len(rel) + 2))\n",
        "    return float(np.sum((2**rel - 1) / discounts))\n",
        "\n",
        "\n",
        "def evaluate_ranking_with_neg_sampling(\n",
        "    model: TwoTowerRecSys,\n",
        "    test_df: pd.DataFrame,\n",
        "    train_pool: pd.DataFrame,\n",
        "    item_table: pd.DataFrame,\n",
        "    user2idx: Dict[int, int],\n",
        "    item2idx: Dict[int, int],\n",
        "    lang2idx: Dict[int, int],\n",
        "    device: str,\n",
        "    k_list=None,\n",
        "    num_negatives: int = 100,\n",
        "):\n",
        "    \"\"\"\n",
        "    More robust ranking evaluation:\n",
        "    For each user in test_df:\n",
        "      - Positives = repos in test_df with target=1.\n",
        "      - Negatives = sample 'num_negatives' items from full catalog that the user\n",
        "        has never positively interacted with (train_pool + test_df).\n",
        "      - Rank positive + sampled negatives with the model.\n",
        "      - Compute Recall@K and NDCG@K for each K.\n",
        "\n",
        "    Returns: dict like\n",
        "      {\n",
        "        \"Recall@5\": value,\n",
        "        \"Recall@10\": value,\n",
        "        \"NDCG@5\": value,\n",
        "        \"NDCG@10\": value,\n",
        "        \"num_users\": n_users_evaluated\n",
        "      }\n",
        "    \"\"\"\n",
        "    if k_list is None:\n",
        "        k_list = [5, 10]\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Pre-build mapping from project_id -> row index in item_table\n",
        "    item_row_map = {pid: idx for idx, pid in enumerate(item_table[ITEM_ID_COL].values)}\n",
        "    all_item_ids = item_table[ITEM_ID_COL].values\n",
        "\n",
        "    # For each user: set of items they have positively interacted with anywhere\n",
        "    user_pos_all = (\n",
        "        pd.concat([train_pool, test_df], ignore_index=True)\n",
        "        .query(f\"{TARGET_COL} == 1\")\n",
        "        .groupby(USER_ID_COL)[ITEM_ID_COL]\n",
        "        .agg(lambda x: set(x.values))\n",
        "    )\n",
        "\n",
        "    # Positives only from test\n",
        "    user_pos_test = (\n",
        "        test_df.query(f\"{TARGET_COL} == 1\")\n",
        "        .groupby(USER_ID_COL)[ITEM_ID_COL]\n",
        "        .agg(lambda x: list(set(x.values)))\n",
        "    )\n",
        "\n",
        "    # Users we can evaluate on (have positives in test and exist in mapping)\n",
        "    eval_users = [\n",
        "        uid for uid in user_pos_test.index\n",
        "        if (uid in user2idx) and (uid in user_pos_all.index)\n",
        "    ]\n",
        "\n",
        "    recalls = {k: [] for k in k_list}\n",
        "    ndcgs = {k: [] for k in k_list}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for uid in eval_users:\n",
        "            pos_items = user_pos_test[uid]\n",
        "            if len(pos_items) == 0:\n",
        "                continue\n",
        "\n",
        "            all_pos = user_pos_all[uid]\n",
        "            seen_items = set(all_pos)\n",
        "\n",
        "            # Candidate negative pool = all items - seen positives\n",
        "            candidate_neg = [pid for pid in all_item_ids if pid not in seen_items]\n",
        "            if len(candidate_neg) == 0:\n",
        "                continue\n",
        "\n",
        "            # Sample negatives\n",
        "            num_neg = min(num_negatives, len(candidate_neg))\n",
        "            neg_items = random.sample(candidate_neg, num_neg)\n",
        "\n",
        "            # Build candidate list: positives + negatives\n",
        "            cand_items = list(pos_items) + neg_items\n",
        "            cand_labels = np.array(\n",
        "                [1] * len(pos_items) + [0] * len(neg_items),\n",
        "                dtype=np.int32,\n",
        "            )\n",
        "\n",
        "            # Map to item_table rows\n",
        "            cand_indices = [item_row_map[pid] for pid in cand_items]\n",
        "            cand_rows = item_table.iloc[cand_indices]\n",
        "\n",
        "            item_ids_raw = cand_rows[ITEM_ID_COL].values\n",
        "            lang_raw = cand_rows[\"language_code\"].values\n",
        "\n",
        "            item_indices = torch.tensor(\n",
        "                [item2idx[i] for i in item_ids_raw],\n",
        "                dtype=torch.long,\n",
        "                device=device,\n",
        "            )\n",
        "            lang_indices = torch.tensor(\n",
        "                [lang2idx[l] for l in lang_raw],\n",
        "                dtype=torch.long,\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "            numerics_tensor = torch.tensor(\n",
        "                cand_rows[NUMERIC_REPO_COLS].astype(float).values,\n",
        "                dtype=torch.float32,\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "            u_idx = user2idx[uid]\n",
        "            user_tensor = torch.tensor([u_idx], dtype=torch.long, device=device)\n",
        "            user_batch = user_tensor.expand(len(cand_items))\n",
        "\n",
        "            logits, _, _ = model(user_batch, item_indices, lang_indices, numerics_tensor)\n",
        "            scores = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "\n",
        "            # Sort candidates by score\n",
        "            ranking = np.argsort(-scores)  # descending\n",
        "            sorted_labels = cand_labels[ranking]\n",
        "\n",
        "            num_pos = float(len(pos_items))\n",
        "\n",
        "            for k in k_list:\n",
        "                k_eff = min(k, len(sorted_labels))\n",
        "                topk_labels = sorted_labels[:k_eff]\n",
        "\n",
        "                # Recall@K: how many positives in top-K divided by total positives\n",
        "                recall_k = float(topk_labels.sum() / num_pos)\n",
        "                recalls[k].append(recall_k)\n",
        "\n",
        "                # NDCG@K\n",
        "                dcg_k = compute_dcg(sorted_labels, k_eff)\n",
        "                # Ideal DCG: all positives ranked at top\n",
        "                ideal_labels = np.sort(cand_labels)[::-1]\n",
        "                idcg_k = compute_dcg(ideal_labels, k_eff)\n",
        "                ndcg_k = dcg_k / idcg_k if idcg_k > 0 else 0.0\n",
        "                ndcgs[k].append(ndcg_k)\n",
        "\n",
        "    metrics = {}\n",
        "    n_users = len(eval_users)\n",
        "    metrics[\"num_users\"] = n_users\n",
        "    for k in k_list:\n",
        "        if len(recalls[k]) > 0:\n",
        "            metrics[f\"Recall@{k}\"] = float(np.mean(recalls[k]))\n",
        "        else:\n",
        "            metrics[f\"Recall@{k}\"] = float(\"nan\")\n",
        "        if len(ndcgs[k]) > 0:\n",
        "            metrics[f\"NDCG@{k}\"] = float(np.mean(ndcgs[k]))\n",
        "        else:\n",
        "            metrics[f\"NDCG@{k}\"] = float(\"nan\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "2Jf5Krd-PUS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 7. MAIN SCRIPT\n",
        "########################################\n",
        "\n",
        "def main():\n",
        "    cfg = Config()\n",
        "    set_seed(cfg.seed)\n",
        "\n",
        "    print(\"Loading raw data...\")\n",
        "    train_bal, train_neg, test_bal, test_neg = load_raw_data(cfg)\n",
        "\n",
        "    train_pool = pd.concat([train_bal, train_neg], ignore_index=True)\n",
        "    test_df = pd.concat([test_bal, test_neg], ignore_index=True)\n",
        "\n",
        "    print(f\"Train pool size: {len(train_pool)}, Test size: {len(test_df)}\")\n",
        "\n",
        "    print(\"Splitting train pool into train and validation by user...\")\n",
        "    train_df_all, val_df = split_train_val_by_user(\n",
        "        train_pool,\n",
        "        train_user_fraction=cfg.train_user_fraction,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "\n",
        "    # IMPORTANT for In-Batch Negatives:\n",
        "    # We only train on POSITIVE interactions. The \"negatives\" are implicitly\n",
        "    # the other items in the batch.\n",
        "    print(\"Filtering training set to keep only POSITIVES (target=1)...\")\n",
        "    train_df = train_df_all[train_df_all[TARGET_COL] == 1].reset_index(drop=True)\n",
        "\n",
        "    # We keep val_df as is (mixed pos/neg) so we can still calculate pointwise AUC\n",
        "    # as a sanity check.\n",
        "\n",
        "    print(f\"Train size (pos only): {len(train_df)}, Val size (mixed): {len(val_df)}\")\n",
        "\n",
        "    print(\"Building ID mappings (users/items/langs)...\")\n",
        "    user2idx, item2idx, lang2idx, idx2user, idx2item, idx2lang = build_id_mappings(\n",
        "        train_pool, test_df\n",
        "    )\n",
        "    num_users = len(user2idx)\n",
        "    num_items = len(item2idx)\n",
        "    num_langs = len(lang2idx)\n",
        "\n",
        "    print(f\"Num users: {num_users}, num items: {num_items}, num langs: {num_langs}\")\n",
        "\n",
        "    print(\"Fitting numeric scalers on training pool...\")\n",
        "    means, stds = compute_numeric_scalers(train_pool)\n",
        "\n",
        "    print(\"Normalizing numeric features...\")\n",
        "    train_df_norm = normalize_numeric_features(train_df, means, stds)\n",
        "    val_df_norm = normalize_numeric_features(val_df, means, stds)\n",
        "    test_df_norm = normalize_numeric_features(test_df, means, stds)\n",
        "\n",
        "    print(\"Creating dataloaders for train and validation...\")\n",
        "    train_loader, val_loader = make_dataloaders(\n",
        "        cfg, train_df_norm, val_df_norm, user2idx, item2idx, lang2idx\n",
        "    )\n",
        "\n",
        "    print(\"Initializing model...\")\n",
        "    model = TwoTowerRecSys(\n",
        "        num_users=num_users,\n",
        "        num_items=num_items,\n",
        "        num_langs=num_langs,\n",
        "        num_numeric_feats=len(NUMERIC_REPO_COLS),\n",
        "        cfg=cfg,\n",
        "    ).to(cfg.device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=cfg.lr,\n",
        "        weight_decay=cfg.weight_decay,\n",
        "    )\n",
        "\n",
        "    best_auc = -1.0\n",
        "\n",
        "    for epoch in range(1, cfg.num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{cfg.num_epochs}\")\n",
        "\n",
        "        # Pass temperature to train function\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, cfg.device, cfg.temperature)\n",
        "        print(f\"Train loss (CrossEntropy): {train_loss:.4f}\")\n",
        "\n",
        "        val_loss, val_auc = evaluate_pointwise(model, val_loader, cfg.device)\n",
        "        print(f\"Val loss (Pointwise): {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "        # Note: best model selection is still based on AUC here, which is fine,\n",
        "        # but ideally we would track Recall@K on validation.\n",
        "        if SKLEARN_AVAILABLE and not math.isnan(val_auc) and val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            torch.save(model.state_dict(), \"best_twotower_model.pt\")\n",
        "            print(\"Saved new best model based on validation AUC.\")\n",
        "\n",
        "    # Load best model (if saved)\n",
        "    if os.path.exists(\"best_twotower_model.pt\"):\n",
        "        model.load_state_dict(torch.load(\"best_twotower_model.pt\", map_location=cfg.device))\n",
        "        print(\"Loaded best model from checkpoint.\")\n",
        "\n",
        "    # Final pointwise evaluation on test set (AUC / loss)\n",
        "    print(\"\\nCreating test DataLoader for pointwise metrics...\")\n",
        "    test_dataset = TwoTowerDataset(test_df_norm, user2idx, item2idx, lang2idx)\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    test_loss, test_auc = evaluate_pointwise(model, test_loader, cfg.device)\n",
        "    print(f\"Test loss: {test_loss:.4f}, Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "    # Robust ranking evaluation with negative sampling (Recall@K, NDCG@K)\n",
        "    print(\"\\nBuilding normalized item feature table for ranking evaluation...\")\n",
        "    item_table_norm = build_item_feature_table_norm(train_pool, test_df, means, stds)\n",
        "\n",
        "    print(\"Evaluating ranking metrics with negative sampling...\")\n",
        "    rank_metrics = evaluate_ranking_with_neg_sampling(\n",
        "        model=model,\n",
        "        test_df=test_df,\n",
        "        train_pool=train_pool,\n",
        "        item_table=item_table_norm,\n",
        "        user2idx=user2idx,\n",
        "        item2idx=item2idx,\n",
        "        lang2idx=lang2idx,\n",
        "        device=cfg.device,\n",
        "        k_list=cfg.eval_k_list,\n",
        "        num_negatives=cfg.eval_num_negatives,\n",
        "    )\n",
        "\n",
        "    print(f\"Ranking evaluation over {rank_metrics['num_users']} users:\")\n",
        "    for k in cfg.eval_k_list:\n",
        "        print(\n",
        "            f\"  Recall@{k}: {rank_metrics.get(f'Recall@{k}', float('nan')):.4f}, \"\n",
        "            f\"NDCG@{k}: {rank_metrics.get(f'NDCG@{k}', float('nan')):.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWcQjRk8GEjv",
        "outputId": "ce64f8bb-b310-4fb7-b97f-397f5988f380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw data...\n",
            "Train pool size: 567339, Test size: 141832\n",
            "Splitting train pool into train and validation by user...\n",
            "Filtering training set to keep only POSITIVES (target=1)...\n",
            "Train size (pos only): 293817, Val size (mixed): 113522\n",
            "Building ID mappings (users/items/langs)...\n",
            "Num users: 10000, num items: 365628, num langs: 119\n",
            "Fitting numeric scalers on training pool...\n",
            "Normalizing numeric features...\n",
            "Creating dataloaders for train and validation...\n",
            "Initializing model...\n",
            "\n",
            "Epoch 1/10\n",
            "Train loss (CrossEntropy): 7.6237\n",
            "Val loss (Pointwise): 0.6839, Val AUC: 0.5002\n",
            "Saved new best model based on validation AUC.\n",
            "\n",
            "Epoch 2/10\n",
            "Train loss (CrossEntropy): 7.2665\n",
            "Val loss (Pointwise): 0.6810, Val AUC: 0.5002\n",
            "Saved new best model based on validation AUC.\n",
            "\n",
            "Epoch 3/10\n",
            "Train loss (CrossEntropy): 6.4256\n",
            "Val loss (Pointwise): 0.6881, Val AUC: 0.4972\n",
            "\n",
            "Epoch 4/10\n",
            "Train loss (CrossEntropy): 5.7954\n",
            "Val loss (Pointwise): 0.7011, Val AUC: 0.4983\n",
            "\n",
            "Epoch 5/10\n",
            "Train loss (CrossEntropy): 5.3687\n",
            "Val loss (Pointwise): 0.7069, Val AUC: 0.5019\n",
            "Saved new best model based on validation AUC.\n",
            "\n",
            "Epoch 6/10\n",
            "Train loss (CrossEntropy): 5.0932\n",
            "Val loss (Pointwise): 0.7104, Val AUC: 0.4989\n",
            "\n",
            "Epoch 7/10\n",
            "Train loss (CrossEntropy): 4.8916\n",
            "Val loss (Pointwise): 0.7119, Val AUC: 0.4961\n",
            "\n",
            "Epoch 8/10\n",
            "Train loss (CrossEntropy): 4.7196\n",
            "Val loss (Pointwise): 0.7113, Val AUC: 0.4996\n",
            "\n",
            "Epoch 9/10\n",
            "Train loss (CrossEntropy): 4.5527\n",
            "Val loss (Pointwise): 0.7097, Val AUC: 0.5053\n",
            "Saved new best model based on validation AUC.\n",
            "\n",
            "Epoch 10/10\n",
            "Train loss (CrossEntropy): 4.3784\n",
            "Val loss (Pointwise): 0.7082, Val AUC: 0.5121\n",
            "Saved new best model based on validation AUC.\n",
            "Loaded best model from checkpoint.\n",
            "\n",
            "Creating test DataLoader for pointwise metrics...\n",
            "Test loss: 0.6539, Test AUC: 0.5784\n",
            "\n",
            "Building normalized item feature table for ranking evaluation...\n",
            "Evaluating ranking metrics with negative sampling...\n",
            "Ranking evaluation over 10000 users:\n",
            "  Recall@5: 0.2887, NDCG@5: 0.4932\n",
            "  Recall@10: 0.4203, NDCG@10: 0.4620\n"
          ]
        }
      ]
    }
  ]
}