{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DK2-hF2d0Tt",
        "outputId": "7838f6b7-0745-444a-9e02-5ff2b1486cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# github_recsys_twotower_with_robust_eval.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Optional: for AUC metric\n",
        "try:\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    SKLEARN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SKLEARN_AVAILABLE = False\n",
        "\n",
        "\n",
        "########################################\n",
        "# 1. CONFIG AND FEATURE DEFINITIONS\n",
        "########################################\n",
        "\n",
        "class Config:\n",
        "    # File paths: adapt these to your actual locations\n",
        "    train_balanced_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/train_balanced.csv\"\n",
        "    train_negative_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/train_negative.csv\"\n",
        "    test_balanced_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/test_balanced.csv\"\n",
        "    test_negative_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/test_negative.csv\"\n",
        "\n",
        "    # Model hyperparameters\n",
        "    user_id_emb_dim = 64\n",
        "    item_id_emb_dim = 64\n",
        "    lang_emb_dim = 16\n",
        "    hidden_dim = 128\n",
        "    embedding_dim = 64  # final tower output dimension\n",
        "\n",
        "    batch_size = 4096\n",
        "    num_epochs = 10\n",
        "    lr = 1e-3\n",
        "    weight_decay = 1e-5\n",
        "\n",
        "    # Contrastive / pointwise loss config\n",
        "    temperature = 0.1  # Scale factor for cosine similarity logits\n",
        "    contrastive_weight = 0.7  # weight for contrastive (InfoNCE) loss\n",
        "    bce_weight = 0.3         # weight for pointwise BCE loss\n",
        "    max_contrastive_negatives = 256  # max sampled explicit negatives per batch for InfoNCE\n",
        "\n",
        "    # Validation split\n",
        "    train_user_fraction = 0.8  # 80% users for train, 20% for validation\n",
        "\n",
        "    # Ranking evaluation config\n",
        "    eval_k_list = [5, 10]\n",
        "    eval_num_negatives = 100  # per user for ranking eval\n",
        "\n",
        "    # CUDA if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Random seeds for reproducibility\n",
        "    seed = 42\n",
        "\n",
        "\n",
        "# Columns in csvs\n",
        "NUMERIC_REPO_COLS = [\n",
        "    \"watchers\", \"commits\", \"issues\", \"pull_requests\",\n",
        "    \"mean_commits_language\", \"max_commits_language\", \"min_commits_language\",\n",
        "    \"std_commits_language\",\n",
        "    \"mean_pull_requests_language\", \"max_pull_requests_language\",\n",
        "    \"min_pull_requests_language\", \"std_pull_requests_language\",\n",
        "    \"mean_issues_language\", \"max_issues_language\", \"min_issues_language\",\n",
        "    \"std_issues_language\",\n",
        "    \"mean_watchers_language\", \"max_watchers_language\",\n",
        "    \"min_watchers_language\", \"std_watchers_language\",\n",
        "    \"events\", \"year\",\n",
        "    \"weight\", \"cp\", \"avg_cp\", \"stddev\",\n",
        "]\n",
        "\n",
        "CATEGORICAL_REPO_COLS = [\n",
        "    \"language_code\",\n",
        "]\n",
        "\n",
        "USER_ID_COL = \"id_user\"\n",
        "ITEM_ID_COL = \"project_id\"\n",
        "TARGET_COL = \"target\""
      ],
      "metadata": {
        "id": "-cedvhnYgedl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 2. DATA LOADING AND PREPROCESSING\n",
        "########################################\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def load_raw_data(cfg: Config):\n",
        "    train_bal = pd.read_csv(cfg.train_balanced_path)\n",
        "    train_neg = pd.read_csv(cfg.train_negative_path)\n",
        "    test_bal = pd.read_csv(cfg.test_balanced_path)\n",
        "    test_neg = pd.read_csv(cfg.test_negative_path)\n",
        "    return train_bal, train_neg, test_bal, test_neg\n",
        "\n",
        "\n",
        "def build_id_mappings(train_pool: pd.DataFrame, test_df: pd.DataFrame):\n",
        "    # Unique users and items across train_pool + test\n",
        "    all_users = pd.concat([train_pool[USER_ID_COL], test_df[USER_ID_COL]]).unique()\n",
        "    all_items = pd.concat([train_pool[ITEM_ID_COL], test_df[ITEM_ID_COL]]).unique()\n",
        "    all_langs = pd.concat([train_pool[\"language_code\"], test_df[\"language_code\"]]).unique()\n",
        "\n",
        "    user2idx = {uid: i for i, uid in enumerate(all_users)}\n",
        "    item2idx = {iid: i for i, iid in enumerate(all_items)}\n",
        "    lang2idx = {lid: i for i, lid in enumerate(all_langs)}\n",
        "\n",
        "    idx2user = {i: uid for uid, i in user2idx.items()}\n",
        "    idx2item = {i: iid for iid, i in item2idx.items()}\n",
        "    idx2lang = {i: lid for lid, i in lang2idx.items()}\n",
        "\n",
        "    return user2idx, item2idx, lang2idx, idx2user, idx2item, idx2lang\n",
        "\n",
        "\n",
        "def compute_numeric_scalers(train_pool: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute mean and std for each numeric repo feature on training pool data.\n",
        "    Returns dicts: col -> (mean, std).\n",
        "    \"\"\"\n",
        "    means = {}\n",
        "    stds = {}\n",
        "    for col in NUMERIC_REPO_COLS:\n",
        "        col_values = train_pool[col].astype(float).values\n",
        "        mean = col_values.mean()\n",
        "        std = col_values.std()\n",
        "        if std < 1e-6:\n",
        "            std = 1.0\n",
        "        means[col] = mean\n",
        "        stds[col] = std\n",
        "    return means, stds\n",
        "\n",
        "\n",
        "def normalize_numeric_features(df: pd.DataFrame, means: Dict[str, float], stds: Dict[str, float]):\n",
        "    df = df.copy()\n",
        "    for col in NUMERIC_REPO_COLS:\n",
        "        df[col] = (df[col].astype(float) - means[col]) / stds[col]\n",
        "    return df\n",
        "\n",
        "\n",
        "def split_train_val_by_user(\n",
        "    train_pool: pd.DataFrame,\n",
        "    train_user_fraction: float,\n",
        "    seed: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Split the training pool into train_df and val_df based on users.\n",
        "    \"\"\"\n",
        "    all_users = train_pool[USER_ID_COL].unique()\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(all_users)\n",
        "\n",
        "    n_train_users = int(len(all_users) * train_user_fraction)\n",
        "    train_users = set(all_users[:n_train_users])\n",
        "    val_users = set(all_users[n_train_users:])\n",
        "\n",
        "    train_df = train_pool[train_pool[USER_ID_COL].isin(train_users)].reset_index(drop=True)\n",
        "    val_df = train_pool[train_pool[USER_ID_COL].isin(val_users)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def build_item_feature_table_norm(\n",
        "    train_pool: pd.DataFrame,\n",
        "    test_df: pd.DataFrame,\n",
        "    means: Dict[str, float],\n",
        "    stds: Dict[str, float],\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a unique, normalized item feature table: one row per project_id.\n",
        "    \"\"\"\n",
        "    all_df = pd.concat([train_pool, test_df], ignore_index=True)\n",
        "    all_df_norm = normalize_numeric_features(all_df, means, stds)\n",
        "\n",
        "    # Keep one row per item; using last occurrence (could also aggregate)\n",
        "    all_df_norm = all_df_norm.sort_values(\"events\")\n",
        "    item_table = all_df_norm.drop_duplicates(subset=[ITEM_ID_COL], keep=\"last\")\n",
        "    item_table = item_table.reset_index(drop=True)\n",
        "    return item_table"
      ],
      "metadata": {
        "id": "ab3vl_J-g_c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 3. DATASET AND DATALOADER\n",
        "########################################\n",
        "\n",
        "class TwoTowerDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        user2idx: Dict[int, int],\n",
        "        item2idx: Dict[int, int],\n",
        "        lang2idx: Dict[int, int],\n",
        "    ):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.user2idx = user2idx\n",
        "        self.item2idx = item2idx\n",
        "        self.lang2idx = lang2idx\n",
        "\n",
        "        # Pre-extract numpy arrays so __getitem__ is fast\n",
        "        self.user_ids = self.df[USER_ID_COL].values\n",
        "        self.item_ids = self.df[ITEM_ID_COL].values\n",
        "        self.lang_codes = self.df[\"language_code\"].values\n",
        "        self.labels = self.df[TARGET_COL].astype(float).values\n",
        "\n",
        "        self.numeric_matrix = self.df[NUMERIC_REPO_COLS].astype(float).values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        uid = self.user_ids[idx]\n",
        "        iid = self.item_ids[idx]\n",
        "        lang = self.lang_codes[idx]\n",
        "        label = self.labels[idx]\n",
        "        numerics = self.numeric_matrix[idx]\n",
        "\n",
        "        u_idx = self.user2idx[uid]\n",
        "        i_idx = self.item2idx[iid]\n",
        "        l_idx = self.lang2idx[lang]\n",
        "\n",
        "        # Convert to tensors\n",
        "        u_idx = torch.tensor(u_idx, dtype=torch.long)\n",
        "        i_idx = torch.tensor(i_idx, dtype=torch.long)\n",
        "        l_idx = torch.tensor(l_idx, dtype=torch.long)\n",
        "        numerics = torch.tensor(numerics, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        return u_idx, i_idx, l_idx, numerics, label\n",
        "\n",
        "\n",
        "def make_dataloaders(\n",
        "    cfg: Config,\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    user2idx: Dict[int, int],\n",
        "    item2idx: Dict[int, int],\n",
        "    lang2idx: Dict[int, int],\n",
        "):\n",
        "    train_dataset = TwoTowerDataset(train_df, user2idx, item2idx, lang2idx)\n",
        "    val_dataset = TwoTowerDataset(val_df, user2idx, item2idx, lang2idx)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "EMNyvx99g_88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 4. MODEL DEFINITIONS\n",
        "########################################\n",
        "\n",
        "class UserTower(nn.Module):\n",
        "    def __init__(self, num_users: int, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(num_users, cfg.user_id_emb_dim)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.user_id_emb_dim, cfg.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(cfg.hidden_dim, cfg.embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, user_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        user_ids: [B]\n",
        "        returns: [B, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = self.user_emb(user_ids)  # [B, user_id_emb_dim]\n",
        "        x = self.mlp(x)              # [B, embedding_dim]\n",
        "        x = nn.functional.normalize(x, p=2, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ItemTower(nn.Module):\n",
        "    def __init__(self, num_items: int, num_langs: int, num_numeric_feats: int, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.item_emb = nn.Embedding(num_items, cfg.item_id_emb_dim)\n",
        "        self.lang_emb = nn.Embedding(num_langs, cfg.lang_emb_dim)\n",
        "\n",
        "        input_dim = cfg.item_id_emb_dim + cfg.lang_emb_dim + num_numeric_feats\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, cfg.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(cfg.hidden_dim, cfg.embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        item_ids: torch.Tensor,     # [B]\n",
        "        lang_ids: torch.Tensor,     # [B]\n",
        "        numeric_feats: torch.Tensor # [B, num_numeric_feats]\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        returns: [B, embedding_dim]\n",
        "        \"\"\"\n",
        "        item_e = self.item_emb(item_ids)    # [B, item_id_emb_dim]\n",
        "        lang_e = self.lang_emb(lang_ids)    # [B, lang_emb_dim]\n",
        "\n",
        "        x = torch.cat([item_e, lang_e, numeric_feats], dim=-1)  # [B, input_dim]\n",
        "        x = self.mlp(x)                                         # [B, embedding_dim]\n",
        "        x = nn.functional.normalize(x, p=2, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TwoTowerRecSys(nn.Module):\n",
        "    def __init__(self, num_users: int, num_items: int, num_langs: int, num_numeric_feats: int, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.user_tower = UserTower(num_users, cfg)\n",
        "        self.item_tower = ItemTower(num_items, num_langs, num_numeric_feats, cfg)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        user_ids: torch.Tensor,\n",
        "        item_ids: torch.Tensor,\n",
        "        lang_ids: torch.Tensor,\n",
        "        numeric_feats: torch.Tensor,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        user_ids: [B]\n",
        "        item_ids: [B]\n",
        "        lang_ids: [B]\n",
        "        numeric_feats: [B, F]\n",
        "\n",
        "        returns:\n",
        "            logits: [B] (dot products)\n",
        "            user_embs: [B, D]\n",
        "            item_embs: [B, D]\n",
        "        \"\"\"\n",
        "        u = self.user_tower(user_ids)                          # [B, D]\n",
        "        v = self.item_tower(item_ids, lang_ids, numeric_feats) # [B, D]\n",
        "\n",
        "        logits = torch.sum(u * v, dim=-1)  # [B]\n",
        "        return logits, u, v"
      ],
      "metadata": {
        "id": "Lcja2BFxhFEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch_contrastive(\n",
        "    model,\n",
        "    train_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    temperature: float = 0.1,\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_examples = 0\n",
        "    ce_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        user_ids, item_ids, lang_ids, numerics, labels = batch\n",
        "\n",
        "        # Use only positives for contrastive\n",
        "        pos_mask = labels > 0.5\n",
        "        if pos_mask.sum() < 2:\n",
        "            continue\n",
        "\n",
        "        user_ids = user_ids[pos_mask].to(device)\n",
        "        item_ids = item_ids[pos_mask].to(device)\n",
        "        lang_ids = lang_ids[pos_mask].to(device)\n",
        "        numerics = numerics[pos_mask].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        _, u_emb, v_emb = model(user_ids, item_ids, lang_ids, numerics)\n",
        "\n",
        "        # In-batch negatives: [B, B]\n",
        "        sim_matrix = torch.matmul(u_emb, v_emb.T)\n",
        "        logits = sim_matrix / temperature\n",
        "\n",
        "        batch_size = user_ids.size(0)\n",
        "        targets = torch.arange(batch_size, device=device, dtype=torch.long)\n",
        "\n",
        "        loss = ce_criterion(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_examples += batch_size\n",
        "\n",
        "    return total_loss / max(total_examples, 1)\n",
        "\n",
        "\n",
        "def train_one_epoch_mixed(\n",
        "    model: TwoTowerRecSys,\n",
        "    train_loader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: str,\n",
        "    temperature: float = 0.1,\n",
        "    contrastive_weight: float = 0.7,\n",
        "    bce_weight: float = 0.3,\n",
        "    max_contrastive_negatives: int = 256,\n",
        "):\n",
        "    \"\"\"\n",
        "    Stage B: mixed objective for finetuning.\n",
        "\n",
        "    - BCE on ALL examples (positives + explicit negatives).\n",
        "    - Contrastive InfoNCE on POSITIVE examples, with additional explicit\n",
        "      negatives sampled from the batch and added to the denominator.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    ce_criterion = nn.CrossEntropyLoss()\n",
        "    bce_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        user_ids, item_ids, lang_ids, numerics, labels = batch\n",
        "\n",
        "        user_ids = user_ids.to(device)\n",
        "        item_ids = item_ids.to(device)\n",
        "        lang_ids = lang_ids.to(device)\n",
        "        numerics = numerics.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits, u_emb, v_emb = model(user_ids, item_ids, lang_ids, numerics)\n",
        "\n",
        "        # 1) Pointwise BCE on all examples\n",
        "        bce_loss = bce_criterion(logits, labels)\n",
        "\n",
        "        # 2) Contrastive on positives only\n",
        "        pos_mask = labels > 0.5\n",
        "        num_pos = int(pos_mask.sum().item())\n",
        "\n",
        "        if num_pos > 1 and contrastive_weight > 0.0:\n",
        "            u_pos = u_emb[pos_mask]  # [P, D]\n",
        "            v_pos = v_emb[pos_mask]  # [P, D]\n",
        "\n",
        "            # All explicit negatives in the batch (label 0)\n",
        "            neg_mask = labels < 0.5\n",
        "            v_neg_all = v_emb[neg_mask]  # [N, D]\n",
        "\n",
        "            if v_neg_all.size(0) > 0 and max_contrastive_negatives > 0:\n",
        "                num_negs = min(max_contrastive_negatives, v_neg_all.size(0))\n",
        "                perm = torch.randperm(v_neg_all.size(0), device=device)[:num_negs]\n",
        "                v_neg = v_neg_all[perm]  # [num_negs, D]\n",
        "                # Candidate items: positives + sampled negatives\n",
        "                all_items = torch.cat([v_pos, v_neg], dim=0)  # [P + num_negs, D]\n",
        "            else:\n",
        "                # Fallback: only positives as in-batch negatives\n",
        "                all_items = v_pos  # [P, D]\n",
        "\n",
        "            # Similarity between positive users and all candidate items\n",
        "            sim_matrix = torch.matmul(u_pos, all_items.T)  # [P, P + num_negs]\n",
        "            ce_logits = sim_matrix / temperature\n",
        "\n",
        "            # Each user i's positive is at index i\n",
        "            targets = torch.arange(num_pos, device=device, dtype=torch.long)\n",
        "            contrastive_loss = ce_criterion(ce_logits, targets)\n",
        "        else:\n",
        "            contrastive_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        # Combine losses\n",
        "        loss = contrastive_weight * contrastive_loss + bce_weight * bce_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = user_ids.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_examples += batch_size\n",
        "\n",
        "    avg_loss = total_loss / max(total_examples, 1)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_pointwise(\n",
        "    model: TwoTowerRecSys,\n",
        "    data_loader: DataLoader,\n",
        "    device: str,\n",
        "):\n",
        "    \"\"\"\n",
        "    Simple pointwise evaluation:\n",
        "    - average loss\n",
        "    - ROC-AUC (if sklearn available)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            user_ids, item_ids, lang_ids, numerics, labels = batch\n",
        "            user_ids = user_ids.to(device)\n",
        "            item_ids = item_ids.to(device)\n",
        "            lang_ids = lang_ids.to(device)\n",
        "            numerics = numerics.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits, _, _ = model(user_ids, item_ids, lang_ids, numerics)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "\n",
        "            batch_size = labels.size(0)\n",
        "            total_loss += loss.item() * batch_size\n",
        "            total_examples += batch_size\n",
        "\n",
        "            all_labels.append(labels.detach().cpu().numpy())\n",
        "            all_probs.append(probs.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / max(total_examples, 1)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_probs = np.concatenate(all_probs, axis=0)\n",
        "\n",
        "    if SKLEARN_AVAILABLE:\n",
        "        try:\n",
        "            auc = roc_auc_score(all_labels, all_probs)\n",
        "        except ValueError:\n",
        "            auc = float(\"nan\")\n",
        "    else:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    return avg_loss, auc"
      ],
      "metadata": {
        "id": "chzSj3NMhG95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 6. ROBUST RANKING METRICS (Recall@K, NDCG@K)\n",
        "########################################\n",
        "\n",
        "def compute_dcg(relevances: np.ndarray, k: int) -> float:\n",
        "    \"\"\"\n",
        "    relevances: binary or graded relevance scores sorted in ranking order.\n",
        "    \"\"\"\n",
        "    rel = relevances[:k]\n",
        "    if len(rel) == 0:\n",
        "        return 0.0\n",
        "    discounts = np.log2(np.arange(2, len(rel) + 2))\n",
        "    return float(np.sum((2**rel - 1) / discounts))\n",
        "\n",
        "\n",
        "def evaluate_ranking_with_neg_sampling(\n",
        "    model: TwoTowerRecSys,\n",
        "    test_df: pd.DataFrame,\n",
        "    train_pool: pd.DataFrame,\n",
        "    item_table: pd.DataFrame,\n",
        "    user2idx: Dict[int, int],\n",
        "    item2idx: Dict[int, int],\n",
        "    lang2idx: Dict[int, int],\n",
        "    device: str,\n",
        "    k_list=None,\n",
        "    num_negatives: int = 100,\n",
        "):\n",
        "    \"\"\"\n",
        "    More robust ranking evaluation:\n",
        "    For each user in test_df:\n",
        "      - Positives = repos in test_df with target=1.\n",
        "      - Negatives = sample 'num_negatives' items from full catalog that the user\n",
        "        has never positively interacted with (train_pool + test_df).\n",
        "      - Rank positive + sampled negatives with the model.\n",
        "      - Compute Recall@K and NDCG@K for each K.\n",
        "\n",
        "    Returns: dict like\n",
        "      {\n",
        "        \"Recall@5\": value,\n",
        "        \"Recall@10\": value,\n",
        "        \"NDCG@5\": value,\n",
        "        \"NDCG@10\": value,\n",
        "        \"num_users\": n_users_evaluated\n",
        "      }\n",
        "    \"\"\"\n",
        "    if k_list is None:\n",
        "        k_list = [5, 10]\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Pre-build mapping from project_id -> row index in item_table\n",
        "    item_row_map = {pid: idx for idx, pid in enumerate(item_table[ITEM_ID_COL].values)}\n",
        "    all_item_ids = item_table[ITEM_ID_COL].values\n",
        "\n",
        "    # For each user: set of items they have positively interacted with anywhere\n",
        "    user_pos_all = (\n",
        "        pd.concat([train_pool, test_df], ignore_index=True)\n",
        "        .query(f\"{TARGET_COL} == 1\")\n",
        "        .groupby(USER_ID_COL)[ITEM_ID_COL]\n",
        "        .agg(lambda x: set(x.values))\n",
        "    )\n",
        "\n",
        "    # Positives only from test\n",
        "    user_pos_test = (\n",
        "        test_df.query(f\"{TARGET_COL} == 1\")\n",
        "        .groupby(USER_ID_COL)[ITEM_ID_COL]\n",
        "        .agg(lambda x: list(set(x.values)))\n",
        "    )\n",
        "\n",
        "    # Users we can evaluate on (have positives in test and exist in mapping)\n",
        "    eval_users = [\n",
        "        uid for uid in user_pos_test.index\n",
        "        if (uid in user2idx) and (uid in user_pos_all.index)\n",
        "    ]\n",
        "\n",
        "    recalls = {k: [] for k in k_list}\n",
        "    ndcgs = {k: [] for k in k_list}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for uid in eval_users:\n",
        "            pos_items = user_pos_test[uid]\n",
        "            if len(pos_items) == 0:\n",
        "                continue\n",
        "\n",
        "            all_pos = user_pos_all[uid]\n",
        "            seen_items = set(all_pos)\n",
        "\n",
        "            # Candidate negative pool = all items - seen positives\n",
        "            candidate_neg = [pid for pid in all_item_ids if pid not in seen_items]\n",
        "            if len(candidate_neg) == 0:\n",
        "                continue\n",
        "\n",
        "            # Sample negatives\n",
        "            num_neg = min(num_negatives, len(candidate_neg))\n",
        "            neg_items = random.sample(candidate_neg, num_neg)\n",
        "\n",
        "            # Build candidate list: positives + negatives\n",
        "            cand_items = list(pos_items) + neg_items\n",
        "            cand_labels = np.array(\n",
        "                [1] * len(pos_items) + [0] * len(neg_items),\n",
        "                dtype=np.int32,\n",
        "            )\n",
        "\n",
        "            # Map to item_table rows\n",
        "            cand_indices = [item_row_map[pid] for pid in cand_items]\n",
        "            cand_rows = item_table.iloc[cand_indices]\n",
        "\n",
        "            item_ids_raw = cand_rows[ITEM_ID_COL].values\n",
        "            lang_raw = cand_rows[\"language_code\"].values\n",
        "\n",
        "            item_indices = torch.tensor(\n",
        "                [item2idx[i] for i in item_ids_raw],\n",
        "                dtype=torch.long,\n",
        "                device=device,\n",
        "            )\n",
        "            lang_indices = torch.tensor(\n",
        "                [lang2idx[l] for l in lang_raw],\n",
        "                dtype=torch.long,\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "            numerics_tensor = torch.tensor(\n",
        "                cand_rows[NUMERIC_REPO_COLS].astype(float).values,\n",
        "                dtype=torch.float32,\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "            u_idx = user2idx[uid]\n",
        "            user_tensor = torch.tensor([u_idx], dtype=torch.long, device=device)\n",
        "            user_batch = user_tensor.expand(len(cand_items))\n",
        "\n",
        "            logits, _, _ = model(user_batch, item_indices, lang_indices, numerics_tensor)\n",
        "            scores = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "\n",
        "            # Sort candidates by score\n",
        "            ranking = np.argsort(-scores)  # descending\n",
        "            sorted_labels = cand_labels[ranking]\n",
        "\n",
        "            num_pos = float(len(pos_items))\n",
        "\n",
        "            for k in k_list:\n",
        "                k_eff = min(k, len(sorted_labels))\n",
        "                topk_labels = sorted_labels[:k_eff]\n",
        "\n",
        "                # Recall@K: how many positives in top-K divided by total positives\n",
        "                recall_k = float(topk_labels.sum() / num_pos)\n",
        "                recalls[k].append(recall_k)\n",
        "\n",
        "                # NDCG@K\n",
        "                dcg_k = compute_dcg(sorted_labels, k_eff)\n",
        "                # Ideal DCG: all positives ranked at top\n",
        "                ideal_labels = np.sort(cand_labels)[::-1]\n",
        "                idcg_k = compute_dcg(ideal_labels, k_eff)\n",
        "                ndcg_k = dcg_k / idcg_k if idcg_k > 0 else 0.0\n",
        "                ndcgs[k].append(ndcg_k)\n",
        "\n",
        "    metrics = {}\n",
        "    n_users = len(eval_users)\n",
        "    metrics[\"num_users\"] = n_users\n",
        "    for k in k_list:\n",
        "        if len(recalls[k]) > 0:\n",
        "            metrics[f\"Recall@{k}\"] = float(np.mean(recalls[k]))\n",
        "        else:\n",
        "            metrics[f\"Recall@{k}\"] = float(\"nan\")\n",
        "        if len(ndcgs[k]) > 0:\n",
        "            metrics[f\"NDCG@{k}\"] = float(np.mean(ndcgs[k]))\n",
        "        else:\n",
        "            metrics[f\"NDCG@{k}\"] = float(\"nan\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "-blfT7bihROa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 7. MAIN SCRIPT\n",
        "########################################\n",
        "\n",
        "def main():\n",
        "    cfg = Config()\n",
        "    set_seed(cfg.seed)\n",
        "\n",
        "    print(\"Loading raw data...\")\n",
        "    train_bal, train_neg, test_bal, test_neg = load_raw_data(cfg)\n",
        "\n",
        "    # Combine balanced + negative splits\n",
        "    train_pool = pd.concat([train_bal, train_neg], ignore_index=True)\n",
        "    test_df = pd.concat([test_bal, test_neg], ignore_index=True)\n",
        "    print(f\"Train pool size: {len(train_pool)}, Test size: {len(test_df)}\")\n",
        "\n",
        "    # Split train pool into train/val *by user*\n",
        "    print(\"Splitting train pool into train and validation by user...\")\n",
        "    train_df_all, val_df = split_train_val_by_user(\n",
        "        train_pool,\n",
        "        train_user_fraction=cfg.train_user_fraction,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "\n",
        "    num_train_pos = int((train_df_all[TARGET_COL] == 1).sum())\n",
        "    num_train_neg = int((train_df_all[TARGET_COL] == 0).sum())\n",
        "    print(\n",
        "        f\"Train size (pos+neg): {len(train_df_all)} \"\n",
        "        f\"[pos={num_train_pos}, neg={num_train_neg}], \"\n",
        "        f\"Val size (mixed): {len(val_df)}\"\n",
        "    )\n",
        "\n",
        "    # Build mappings over all users/items/langs present in train+test\n",
        "    print(\"Building ID mappings (users/items/langs)...\")\n",
        "    user2idx, item2idx, lang2idx, idx2user, idx2item, idx2lang = build_id_mappings(\n",
        "        train_pool, test_df\n",
        "    )\n",
        "    num_users = len(user2idx)\n",
        "    num_items = len(item2idx)\n",
        "    num_langs = len(lang2idx)\n",
        "    print(f\"Num users: {num_users}, num items: {num_items}, num langs: {num_langs}\")\n",
        "\n",
        "    # Fit numeric feature scalers on the entire training pool\n",
        "    print(\"Fitting numeric scalers on training pool...\")\n",
        "    means, stds = compute_numeric_scalers(train_pool)\n",
        "\n",
        "    # Create POSITIVE-ONLY and FULL views of the training data\n",
        "    train_df_pos = train_df_all[train_df_all[TARGET_COL] == 1].reset_index(drop=True)\n",
        "    train_df_full = train_df_all.reset_index(drop=True)\n",
        "\n",
        "    print(\"Normalizing numeric features...\")\n",
        "    train_df_pos_norm = normalize_numeric_features(train_df_pos, means, stds)\n",
        "    train_df_full_norm = normalize_numeric_features(train_df_full, means, stds)\n",
        "    val_df_norm = normalize_numeric_features(val_df, means, stds)\n",
        "    test_df_norm = normalize_numeric_features(test_df, means, stds)\n",
        "\n",
        "    # Create TWO train loaders:\n",
        "    #  - train_loader_contrastive: positive-only for Stage A\n",
        "    #  - train_loader_full: pos+neg for Stage B (mixed loss)\n",
        "    print(\"Creating dataloaders for Stage A (contrastive) and Stage B (mixed)...\")\n",
        "    train_loader_contrastive, _ = make_dataloaders(\n",
        "        cfg, train_df_pos_norm, val_df_norm, user2idx, item2idx, lang2idx\n",
        "    )\n",
        "    train_loader_full, val_loader = make_dataloaders(\n",
        "        cfg, train_df_full_norm, val_df_norm, user2idx, item2idx, lang2idx\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing model...\")\n",
        "    model = TwoTowerRecSys(\n",
        "        num_users=num_users,\n",
        "        num_items=num_items,\n",
        "        num_langs=num_langs,\n",
        "        num_numeric_feats=len(NUMERIC_REPO_COLS),\n",
        "        cfg=cfg,\n",
        "    ).to(cfg.device)\n",
        "\n",
        "    # -------------------------\n",
        "    # Stage A: pure contrastive\n",
        "    # -------------------------\n",
        "    pretrain_lr = cfg.lr\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    num_pretrain_epochs = 5  # you can tune this\n",
        "    best_stageA_auc = -1.0\n",
        "\n",
        "    for epoch in range(1, num_pretrain_epochs + 1):\n",
        "        print(f\"\\n[Stage A] Contrastive pretraining - Epoch {epoch}/{num_pretrain_epochs}\")\n",
        "\n",
        "        train_loss = train_one_epoch_contrastive(\n",
        "            model=model,\n",
        "            train_loader=train_loader_contrastive,\n",
        "            optimizer=optimizer,\n",
        "            device=cfg.device,\n",
        "            temperature=cfg.temperature,\n",
        "        )\n",
        "        print(f\"Train loss (contrastive): {train_loss:.4f}\")\n",
        "\n",
        "        # Pointwise val AUC as a sanity check\n",
        "        val_loss, val_auc = evaluate_pointwise(model, val_loader, cfg.device)\n",
        "        print(f\"Val loss (pointwise): {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "        if SKLEARN_AVAILABLE and not math.isnan(val_auc) and val_auc > best_stageA_auc:\n",
        "            best_stageA_auc = val_auc\n",
        "            torch.save(model.state_dict(), \"best_twotower_stageA.pt\")\n",
        "            print(\"Saved Stage A best model based on validation AUC.\")\n",
        "\n",
        "    # Load best Stage A model\n",
        "    if os.path.exists(\"best_twotower_stageA.pt\"):\n",
        "        print(\"\\nLoading Stage A best checkpoint for finetuning...\")\n",
        "        model.load_state_dict(torch.load(\"best_twotower_stageA.pt\", map_location=cfg.device))\n",
        "    else:\n",
        "        print(\"\\nWARNING: Stage A checkpoint not found, proceeding with current model.\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Stage B: mixed finetuning\n",
        "    # -------------------------\n",
        "    # Use smaller LR for finetuning\n",
        "    finetune_lr = cfg.lr * 0.1  # e.g., 1e-4 if cfg.lr is 1e-3\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=finetune_lr, weight_decay=cfg.weight_decay\n",
        "    )\n",
        "\n",
        "    num_finetune_epochs = 2  # short finetune\n",
        "    best_stageB_auc = -1.0\n",
        "\n",
        "    for epoch in range(1, num_finetune_epochs + 1):\n",
        "        print(f\"\\n[Stage B] Mixed finetune - Epoch {epoch}/{num_finetune_epochs}\")\n",
        "\n",
        "        train_loss = train_one_epoch_mixed(\n",
        "            model=model,\n",
        "            train_loader=train_loader_full,\n",
        "            optimizer=optimizer,\n",
        "            device=cfg.device,\n",
        "            temperature=cfg.temperature,\n",
        "            contrastive_weight=cfg.contrastive_weight,\n",
        "            bce_weight=cfg.bce_weight,\n",
        "            max_contrastive_negatives=cfg.max_contrastive_negatives,\n",
        "        )\n",
        "        print(f\"Train loss (mixed contrastive+BCE): {train_loss:.4f}\")\n",
        "\n",
        "        val_loss, val_auc = evaluate_pointwise(model, val_loader, cfg.device)\n",
        "        print(f\"Val loss (pointwise): {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "        if SKLEARN_AVAILABLE and not math.isnan(val_auc) and val_auc > best_stageB_auc:\n",
        "            best_stageB_auc = val_auc\n",
        "            torch.save(model.state_dict(), \"best_twotower_stageAB.pt\")\n",
        "            print(\"Saved Stage B best model based on validation AUC.\")\n",
        "\n",
        "    # Decide which checkpoint to use for final test: prefer Stage B if it exists\n",
        "    final_ckpt_path = \"best_twotower_stageAB.pt\"\n",
        "    if not os.path.exists(final_ckpt_path):\n",
        "        final_ckpt_path = \"best_twotower_stageA.pt\"\n",
        "\n",
        "    if os.path.exists(final_ckpt_path):\n",
        "        print(f\"\\nLoading final best model from checkpoint: {final_ckpt_path}\")\n",
        "        model.load_state_dict(torch.load(final_ckpt_path, map_location=cfg.device))\n",
        "    else:\n",
        "        print(\"\\nWARNING: No checkpoint found; using current in-memory model for test.\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Final test evaluation\n",
        "    # -------------------------\n",
        "    print(\"\\nCreating test DataLoader for pointwise metrics...\")\n",
        "    test_dataset = TwoTowerDataset(test_df_norm, user2idx, item2idx, lang2idx)\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    test_loss, test_auc = evaluate_pointwise(model, test_loader, cfg.device)\n",
        "    print(f\"Test loss: {test_loss:.4f}, Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "    print(\"\\nBuilding normalized item feature table for ranking evaluation...\")\n",
        "    item_table_norm = build_item_feature_table_norm(train_pool, test_df, means, stds)\n",
        "\n",
        "    print(\"Evaluating ranking metrics with negative sampling...\")\n",
        "    rank_metrics = evaluate_ranking_with_neg_sampling(\n",
        "        model=model,\n",
        "        test_df=test_df,\n",
        "        train_pool=train_pool,\n",
        "        item_table=item_table_norm,\n",
        "        user2idx=user2idx,\n",
        "        item2idx=item2idx,\n",
        "        lang2idx=lang2idx,\n",
        "        device=cfg.device,\n",
        "        k_list=cfg.eval_k_list,\n",
        "        num_negatives=cfg.eval_num_negatives,\n",
        "    )\n",
        "\n",
        "    print(f\"Ranking evaluation over {rank_metrics['num_users']} users:\")\n",
        "    for k in cfg.eval_k_list:\n",
        "        print(\n",
        "            f\"  Recall@{k}: {rank_metrics.get(f'Recall@{k}', float('nan')):.4f}, \"\n",
        "            f\"NDCG@{k}: {rank_metrics.get(f'NDCG@{k}', float('nan')):.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHn1HkNZhTn_",
        "outputId": "e6d0bbe9-d215-45df-f04c-dd4bb4484ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw data...\n",
            "Train pool size: 567339, Test size: 141832\n",
            "Splitting train pool into train and validation by user...\n",
            "Train size (pos+neg): 453817 [pos=293817, neg=160000], Val size (mixed): 113522\n",
            "Building ID mappings (users/items/langs)...\n",
            "Num users: 10000, num items: 365628, num langs: 119\n",
            "Fitting numeric scalers on training pool...\n",
            "Normalizing numeric features...\n",
            "Creating dataloaders for Stage A (contrastive) and Stage B (mixed)...\n",
            "Initializing model...\n",
            "\n",
            "[Stage A] Contrastive pretraining - Epoch 1/5\n",
            "Train loss (contrastive): 8.3370\n",
            "Val loss (pointwise): 0.6789, Val AUC: 0.5001\n",
            "Saved Stage A best model based on validation AUC.\n",
            "\n",
            "[Stage A] Contrastive pretraining - Epoch 2/5\n",
            "Train loss (contrastive): 8.2596\n",
            "Val loss (pointwise): 0.6860, Val AUC: 0.4984\n",
            "\n",
            "[Stage A] Contrastive pretraining - Epoch 3/5\n",
            "Train loss (contrastive): 7.9308\n",
            "Val loss (pointwise): 0.6801, Val AUC: 0.4986\n",
            "\n",
            "[Stage A] Contrastive pretraining - Epoch 4/5\n",
            "Train loss (contrastive): 7.3453\n",
            "Val loss (pointwise): 0.6842, Val AUC: 0.4945\n",
            "\n",
            "[Stage A] Contrastive pretraining - Epoch 5/5\n",
            "Train loss (contrastive): 6.8521\n",
            "Val loss (pointwise): 0.6918, Val AUC: 0.4964\n",
            "\n",
            "Loading Stage A best checkpoint for finetuning...\n",
            "\n",
            "[Stage B] Mixed finetune - Epoch 1/2\n",
            "Train loss (mixed contrastive+BCE): 5.7601\n",
            "Val loss (pointwise): 0.6523, Val AUC: 0.5766\n",
            "Saved Stage B best model based on validation AUC.\n",
            "\n",
            "[Stage B] Mixed finetune - Epoch 2/2\n",
            "Train loss (mixed contrastive+BCE): 5.7473\n",
            "Val loss (pointwise): 0.6476, Val AUC: 0.5973\n",
            "Saved Stage B best model based on validation AUC.\n",
            "\n",
            "Loading final best model from checkpoint: best_twotower_stageAB.pt\n",
            "\n",
            "Creating test DataLoader for pointwise metrics...\n",
            "Test loss: 0.6475, Test AUC: 0.5977\n",
            "\n",
            "Building normalized item feature table for ranking evaluation...\n",
            "Evaluating ranking metrics with negative sampling...\n",
            "Ranking evaluation over 10000 users:\n",
            "  Recall@5: 0.1177, NDCG@5: 0.1927\n",
            "  Recall@10: 0.2134, NDCG@10: 0.2074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8XJSVH07iCFE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}