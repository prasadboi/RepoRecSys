{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb5d5dd5",
        "outputId": "dc3389af-97fd-4bb4-ff6f-6c2542a83c4e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# github_recsys_twotower_with_robust_eval.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Optional: for AUC metric\n",
        "try:\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    SKLEARN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SKLEARN_AVAILABLE = False\n",
        "\n",
        "\n",
        "########################################\n",
        "# 1. CONFIG AND FEATURE DEFINITIONS\n",
        "########################################\n",
        "\n",
        "class Config:\n",
        "    # File paths: adapt these to your actual locations\n",
        "    train_balanced_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/train_balanced.csv\"\n",
        "    train_negative_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/train_negative.csv\"\n",
        "    test_balanced_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/test_balanced.csv\"\n",
        "    test_negative_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/test_negative.csv\"\n",
        "\n",
        "    # Model hyperparameters\n",
        "    user_id_emb_dim = 64\n",
        "    item_id_emb_dim = 64\n",
        "    lang_emb_dim = 16\n",
        "    hidden_dim = 128\n",
        "    embedding_dim = 64  # final tower output dimension\n",
        "\n",
        "    dropout = 0.0\n",
        "\n",
        "    batch_size = 4096\n",
        "    num_epochs = 2\n",
        "    lr = 1e-3\n",
        "    weight_decay = 1e-5\n",
        "\n",
        "    # Contrastive Loss Config\n",
        "    temperature = 0.1  # Scale factor for cosine similarity logits\n",
        "\n",
        "    # Validation split\n",
        "    train_user_fraction = 0.8  # 80% users for train, 20% for validation\n",
        "\n",
        "    # Ranking evaluation config\n",
        "    eval_k_list = [5, 10]\n",
        "    eval_num_negatives = 100  # per user for ranking eval\n",
        "\n",
        "    # CUDA if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Random seeds for reproducibility\n",
        "    seed = 42\n",
        "\n",
        "    # Debug options\n",
        "    debug_small_train = False   # set True to overfit tiny subset\n",
        "    debug_small_train_size = 2000  # number of positive rows to keep\n",
        "\n",
        "    # === README text embedding config (Stage 4) ===\n",
        "    use_readme_text = True  # toggle this to turn text features on/off\n",
        "\n",
        "    # Paths to the outputs from build_readme_embeddings.py (Stage 4)\n",
        "    readme_emb_npy_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/features/project_readme_embeddings.npy\"\n",
        "    readme_emb_index_path = \"/content/drive/MyDrive/Project_Work/RepoRecSys/data/features/project_readme_embeddings_index.csv\"\n",
        "\n",
        "    # How to use the text embeddings inside the item tower\n",
        "    readme_text_hidden_dim = 64   # projection size for text branch\n",
        "    freeze_text_emb = True        # set False if you want to fine-tune them\n",
        "\n",
        "# Columns in csvs\n",
        "NUMERIC_REPO_COLS = [\n",
        "    \"watchers\", \"commits\", \"issues\", \"pull_requests\",\n",
        "    \"mean_commits_language\", \"max_commits_language\", \"min_commits_language\",\n",
        "    \"std_commits_language\",\n",
        "    \"mean_pull_requests_language\", \"max_pull_requests_language\",\n",
        "    \"min_pull_requests_language\", \"std_pull_requests_language\",\n",
        "    \"mean_issues_language\", \"max_issues_language\", \"min_issues_language\",\n",
        "    \"std_issues_language\",\n",
        "    \"mean_watchers_language\", \"max_watchers_language\",\n",
        "    \"min_watchers_language\", \"std_watchers_language\",\n",
        "    \"events\", \"year\",\n",
        "    \"weight\", \"cp\", \"avg_cp\", \"stddev\",\n",
        "]\n",
        "\n",
        "CATEGORICAL_REPO_COLS = [\n",
        "    \"language_code\",\n",
        "]\n",
        "\n",
        "USER_ID_COL = \"id_user\"\n",
        "ITEM_ID_COL = \"project_id\"\n",
        "TARGET_COL = \"target\""
      ],
      "metadata": {
        "id": "9Et7DSEWD5Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 2. DATA LOADING AND PREPROCESSING\n",
        "########################################\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def load_raw_data(cfg: Config):\n",
        "    train_bal = pd.read_csv(cfg.train_balanced_path)\n",
        "    train_neg = pd.read_csv(cfg.train_negative_path)\n",
        "    test_bal = pd.read_csv(cfg.test_balanced_path)\n",
        "    test_neg = pd.read_csv(cfg.test_negative_path)\n",
        "    return train_bal, train_neg, test_bal, test_neg\n",
        "\n",
        "\n",
        "def build_id_mappings(train_pool: pd.DataFrame, test_df: pd.DataFrame):\n",
        "    # Unique users and items across train_pool + test\n",
        "    all_users = pd.concat([train_pool[USER_ID_COL], test_df[USER_ID_COL]]).unique()\n",
        "    all_items = pd.concat([train_pool[ITEM_ID_COL], test_df[ITEM_ID_COL]]).unique()\n",
        "    all_langs = pd.concat([train_pool[\"language_code\"], test_df[\"language_code\"]]).unique()\n",
        "\n",
        "    user2idx = {uid: i for i, uid in enumerate(all_users)}\n",
        "    item2idx = {iid: i for i, iid in enumerate(all_items)}\n",
        "    lang2idx = {lid: i for i, lid in enumerate(all_langs)}\n",
        "\n",
        "    idx2user = {i: uid for uid, i in user2idx.items()}\n",
        "    idx2item = {i: iid for iid, i in item2idx.items()}\n",
        "    idx2lang = {i: lid for lid, i in lang2idx.items()}\n",
        "\n",
        "    return user2idx, item2idx, lang2idx, idx2user, idx2item, idx2lang\n",
        "\n",
        "\n",
        "def compute_numeric_scalers(train_pool: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Compute mean and std for each numeric repo feature on training pool data.\n",
        "    Returns dicts: col -> (mean, std).\n",
        "    \"\"\"\n",
        "    means = {}\n",
        "    stds = {}\n",
        "    for col in NUMERIC_REPO_COLS:\n",
        "        col_values = train_pool[col].astype(float).values\n",
        "        mean = col_values.mean()\n",
        "        std = col_values.std()\n",
        "        if std < 1e-6:\n",
        "            std = 1.0\n",
        "        means[col] = mean\n",
        "        stds[col] = std\n",
        "    return means, stds\n",
        "\n",
        "\n",
        "def normalize_numeric_features(df: pd.DataFrame, means: Dict[str, float], stds: Dict[str, float]):\n",
        "    df = df.copy()\n",
        "    for col in NUMERIC_REPO_COLS:\n",
        "        df[col] = (df[col].astype(float) - means[col]) / stds[col]\n",
        "    return df\n",
        "\n",
        "\n",
        "def split_train_val_by_user(\n",
        "    train_pool: pd.DataFrame,\n",
        "    train_user_fraction: float,\n",
        "    seed: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Split the training pool into train_df and val_df based on users.\n",
        "    \"\"\"\n",
        "    all_users = train_pool[USER_ID_COL].unique()\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(all_users)\n",
        "\n",
        "    n_train_users = int(len(all_users) * train_user_fraction)\n",
        "    train_users = set(all_users[:n_train_users])\n",
        "    val_users = set(all_users[n_train_users:])\n",
        "\n",
        "    train_df = train_pool[train_pool[USER_ID_COL].isin(train_users)].reset_index(drop=True)\n",
        "    val_df = train_pool[train_pool[USER_ID_COL].isin(val_users)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def build_item_feature_table_norm(\n",
        "    train_pool: pd.DataFrame,\n",
        "    test_df: pd.DataFrame,\n",
        "    means: Dict[str, float],\n",
        "    stds: Dict[str, float],\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a unique, normalized item feature table: one row per project_id.\n",
        "    \"\"\"\n",
        "    all_df = pd.concat([train_pool, test_df], ignore_index=True)\n",
        "    all_df_norm = normalize_numeric_features(all_df, means, stds)\n",
        "\n",
        "    # Keep one row per item; using last occurrence (could also aggregate)\n",
        "    all_df_norm = all_df_norm.sort_values(\"events\")\n",
        "    item_table = all_df_norm.drop_duplicates(subset=[ITEM_ID_COL], keep=\"last\")\n",
        "    item_table = item_table.reset_index(drop=True)\n",
        "    return item_table\n",
        "\n",
        "def get_readme_project_ids(cfg: Config) -> set[int]:\n",
        "    \"\"\"\n",
        "    Return the set of project_ids that have a README embedding.\n",
        "    \"\"\"\n",
        "    if not cfg.use_readme_text:\n",
        "        return set()\n",
        "\n",
        "    idx_df = pd.read_csv(cfg.readme_emb_index_path)\n",
        "    # make sure we drop any NaNs and cast to int\n",
        "    pids = idx_df[\"project_id\"].dropna().astype(\"int64\").unique().tolist()\n",
        "    return set(int(pid) for pid in pids)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_item_text_embeddings(cfg: Config, item2idx: dict) -> tuple[torch.Tensor, int]:\n",
        "    \"\"\"\n",
        "    Build an [num_items, text_dim] matrix where each row is the README embedding\n",
        "    for that item, aligned with item2idx. Items without README get all zeros.\n",
        "\n",
        "    Returns:\n",
        "        item_text_emb_tensor: torch.FloatTensor[num_items, text_dim]\n",
        "        text_dim: int\n",
        "    \"\"\"\n",
        "    if not cfg.use_readme_text:\n",
        "        return None, 0\n",
        "\n",
        "    # Load index and embedding matrix produced in Stage 4\n",
        "    idx_df = pd.read_csv(cfg.readme_emb_index_path)\n",
        "    emb = np.load(cfg.readme_emb_npy_path)  # [N_text_items, D]\n",
        "    text_dim = emb.shape[1]\n",
        "\n",
        "    # Map project_id -> row_idx into emb\n",
        "    row_by_pid = {\n",
        "        int(pid): int(row_idx)\n",
        "        for pid, row_idx in zip(idx_df[\"project_id\"], idx_df[\"row_idx\"])\n",
        "    }\n",
        "\n",
        "    num_items = len(item2idx)\n",
        "    item_text_emb = np.zeros((num_items, text_dim), dtype=np.float32)\n",
        "\n",
        "    missing = 0\n",
        "    for raw_project_id, item_idx in item2idx.items():\n",
        "        row_idx = row_by_pid.get(int(raw_project_id))\n",
        "        if row_idx is not None:\n",
        "            item_text_emb[item_idx] = emb[row_idx]\n",
        "        else:\n",
        "            # no README embedding available for this project_id\n",
        "            missing += 1\n",
        "\n",
        "    print(f\"[Text] Loaded README embeddings for {num_items - missing} / {num_items} items \"\n",
        "          f\"({100.0 * (num_items - missing) / num_items:.2f}%).\")\n",
        "\n",
        "    item_text_emb_tensor = torch.from_numpy(item_text_emb)\n",
        "    return item_text_emb_tensor, text_dim\n"
      ],
      "metadata": {
        "id": "t0AQ8oxwFsWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 3. DATASET AND DATALOADER\n",
        "########################################\n",
        "\n",
        "class TwoTowerDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        user2idx: Dict[int, int],\n",
        "        item2idx: Dict[int, int],\n",
        "        lang2idx: Dict[int, int],\n",
        "    ):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.user2idx = user2idx\n",
        "        self.item2idx = item2idx\n",
        "        self.lang2idx = lang2idx\n",
        "\n",
        "        # Pre-extract numpy arrays so __getitem__ is fast\n",
        "        self.user_ids = self.df[USER_ID_COL].values\n",
        "        self.item_ids = self.df[ITEM_ID_COL].values\n",
        "        self.lang_codes = self.df[\"language_code\"].values\n",
        "        self.labels = self.df[TARGET_COL].astype(float).values\n",
        "\n",
        "        base_numeric = self.df[NUMERIC_REPO_COLS].astype(float).values\n",
        "        has_readme = self.df[\"has_readme\"].astype(float).values.reshape(-1, 1)\n",
        "        self.numeric_matrix = np.concatenate([base_numeric, has_readme], axis=1)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        uid = self.user_ids[idx]\n",
        "        iid = self.item_ids[idx]\n",
        "        lang = self.lang_codes[idx]\n",
        "        label = self.labels[idx]\n",
        "        numerics = self.numeric_matrix[idx]\n",
        "\n",
        "        u_idx = self.user2idx[uid]\n",
        "        i_idx = self.item2idx[iid]\n",
        "        l_idx = self.lang2idx[lang]\n",
        "\n",
        "        # Convert to tensors\n",
        "        u_idx = torch.tensor(u_idx, dtype=torch.long)\n",
        "        i_idx = torch.tensor(i_idx, dtype=torch.long)\n",
        "        l_idx = torch.tensor(l_idx, dtype=torch.long)\n",
        "        numerics = torch.tensor(numerics, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        return u_idx, i_idx, l_idx, numerics, label\n",
        "\n",
        "\n",
        "def make_dataloaders(\n",
        "    cfg: Config,\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    user2idx: Dict[int, int],\n",
        "    item2idx: Dict[int, int],\n",
        "    lang2idx: Dict[int, int],\n",
        "):\n",
        "    train_dataset = TwoTowerDataset(train_df, user2idx, item2idx, lang2idx)\n",
        "    val_dataset = TwoTowerDataset(val_df, user2idx, item2idx, lang2idx)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "FJP3C0LfFyMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 4. MODEL DEFINITIONS\n",
        "########################################\n",
        "\n",
        "class UserTower(nn.Module):\n",
        "    \"\"\"\n",
        "    User tower with:\n",
        "      - user ID embedding\n",
        "      - 2-layer MLP + dropout\n",
        "      - L2-normalized output\n",
        "    \"\"\"\n",
        "    def __init__(self, num_users: int, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(num_users, cfg.user_id_emb_dim)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.user_id_emb_dim, cfg.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(cfg.hidden_dim, cfg.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(cfg.hidden_dim, cfg.embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, user_ids: torch.Tensor) -> torch.Tensor:\n",
        "        # [B] -> [B, user_id_emb_dim]\n",
        "        x = self.user_emb(user_ids)\n",
        "\n",
        "        # [B, user_id_emb_dim] -> [B, embedding_dim]\n",
        "        x = self.mlp(x)\n",
        "\n",
        "        # Normalize to keep dot product â‰ˆ cosine similarity\n",
        "        x = nn.functional.normalize(x, p=2, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class ItemTower(nn.Module):\n",
        "    \"\"\"\n",
        "    Item tower with:\n",
        "      - item ID embedding\n",
        "      - language embedding\n",
        "      - small MLP on numeric features\n",
        "      - optional README text embedding branch\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_items: int,\n",
        "        num_langs: int,\n",
        "        num_numeric_feats: int,\n",
        "        text_dim: int,\n",
        "        item_text_emb_matrix: torch.Tensor | None,\n",
        "        cfg: Config,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.use_text = cfg.use_readme_text and (text_dim > 0) and (item_text_emb_matrix is not None)\n",
        "\n",
        "        # ID + language embeddings (as before)\n",
        "        self.item_emb = nn.Embedding(num_items, cfg.item_id_emb_dim)\n",
        "        self.lang_emb = nn.Embedding(num_langs, cfg.lang_emb_dim)\n",
        "\n",
        "        # Numeric features MLP (Stage 3)\n",
        "        self.num_mlp = nn.Sequential(\n",
        "            nn.Linear(num_numeric_feats, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Optional text embedding branch\n",
        "        if self.use_text:\n",
        "            # Embedding table indexed by item_id (same indices as item_emb)\n",
        "            # from_pretrained will move weights to correct device automatically\n",
        "            self.text_emb = nn.Embedding.from_pretrained(\n",
        "                item_text_emb_matrix.float(), freeze=cfg.freeze_text_emb\n",
        "            )\n",
        "\n",
        "            self.text_proj = nn.Sequential(\n",
        "                nn.Linear(text_dim, cfg.readme_text_hidden_dim),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "            text_output_dim = cfg.readme_text_hidden_dim\n",
        "        else:\n",
        "            self.text_emb = None\n",
        "            self.text_proj = None\n",
        "            text_output_dim = 0\n",
        "\n",
        "        # Fused MLP: concat [item_emb, lang_emb, num_e, text_e]\n",
        "        fused_input_dim = cfg.item_id_emb_dim + cfg.lang_emb_dim + 32 + text_output_dim\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(fused_input_dim, cfg.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(cfg.hidden_dim, cfg.embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        item_ids: torch.Tensor,\n",
        "        lang_ids: torch.Tensor,\n",
        "        numeric_feats: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        item_e = self.item_emb(item_ids)       # [B, item_id_emb_dim]\n",
        "        lang_e = self.lang_emb(lang_ids)       # [B, lang_emb_dim]\n",
        "        num_e = self.num_mlp(numeric_feats)    # [B, 32]\n",
        "\n",
        "        feats = [item_e, lang_e, num_e]\n",
        "\n",
        "        if self.use_text:\n",
        "            # text_emb: [B, text_dim] -> [B, text_hidden_dim]\n",
        "            text_raw = self.text_emb(item_ids)\n",
        "            text_e = self.text_proj(text_raw)\n",
        "            feats.append(text_e)\n",
        "\n",
        "        x = torch.cat(feats, dim=-1)           # [B, fused_input_dim]\n",
        "        x = self.mlp(x)                        # [B, embedding_dim]\n",
        "        x = nn.functional.normalize(x, p=2, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TwoTowerRecSys(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_users: int,\n",
        "        num_items: int,\n",
        "        num_langs: int,\n",
        "        num_numeric_feats: int,\n",
        "        text_dim: int,\n",
        "        item_text_emb_matrix: torch.Tensor | None,\n",
        "        cfg: Config,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.user_tower = UserTower(num_users, cfg)\n",
        "        self.item_tower = ItemTower(\n",
        "            num_items=num_items,\n",
        "            num_langs=num_langs,\n",
        "            num_numeric_feats=num_numeric_feats,\n",
        "            text_dim=text_dim,\n",
        "            item_text_emb_matrix=item_text_emb_matrix,\n",
        "            cfg=cfg,\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        user_ids: torch.Tensor,\n",
        "        item_ids: torch.Tensor,\n",
        "        lang_ids: torch.Tensor,\n",
        "        numeric_feats: torch.Tensor,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        user_ids: [B]\n",
        "        item_ids: [B]\n",
        "        lang_ids: [B]\n",
        "        numeric_feats: [B, F]\n",
        "\n",
        "        returns:\n",
        "            logits: [B] (dot products)\n",
        "            user_embs: [B, D]\n",
        "            item_embs: [B, D]\n",
        "        \"\"\"\n",
        "        u = self.user_tower(user_ids)                          # [B, D]\n",
        "        v = self.item_tower(item_ids, lang_ids, numeric_feats) # [B, D]\n",
        "\n",
        "        logits = torch.sum(u * v, dim=-1)  # [B]\n",
        "        return logits, u, v"
      ],
      "metadata": {
        "id": "gFhKqCa3F22R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(\n",
        "    model: TwoTowerRecSys,\n",
        "    train_loader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: str,\n",
        "    temperature: float = 0.1,\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    # Use CrossEntropyLoss for In-Batch Negatives (Contrastive Learning)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # Unpack batch\n",
        "        # Note: In this mode, we expect 'batch' to contain only POSITIVE pairs.\n",
        "        # 'labels' from the dataset are ignored because the targets are implicit (the diagonal).\n",
        "        user_ids, item_ids, lang_ids, numerics, _ = batch\n",
        "\n",
        "        user_ids = user_ids.to(device)\n",
        "        item_ids = item_ids.to(device)\n",
        "        lang_ids = lang_ids.to(device)\n",
        "        numerics = numerics.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get embeddings from the model\n",
        "        # forward() returns: logits, user_embs, item_embs\n",
        "        # We ignore the pointwise 'logits' and use the embeddings directly.\n",
        "        _, u_emb, v_emb = model(user_ids, item_ids, lang_ids, numerics)\n",
        "\n",
        "        # Compute Similarity Matrix: [Batch_Size, Batch_Size]\n",
        "        # Both u_emb and v_emb are L2 normalized in the model, so this is Cosine Similarity.\n",
        "        sim_matrix = torch.matmul(u_emb, v_emb.T)\n",
        "\n",
        "        # Scale by temperature to sharpen the distribution\n",
        "        logits = sim_matrix / temperature\n",
        "\n",
        "        # The target for user i is item i (the diagonal)\n",
        "        batch_size = user_ids.size(0)\n",
        "        targets = torch.arange(batch_size, device=device, dtype=torch.long)\n",
        "\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_examples += batch_size\n",
        "\n",
        "    avg_loss = total_loss / max(total_examples, 1)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def evaluate_pointwise(\n",
        "    model: TwoTowerRecSys,\n",
        "    data_loader: DataLoader,\n",
        "    device: str,\n",
        "):\n",
        "    \"\"\"\n",
        "    Simple pointwise evaluation:\n",
        "    - average loss\n",
        "    - ROC-AUC (if sklearn available)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            user_ids, item_ids, lang_ids, numerics, labels = batch\n",
        "            user_ids = user_ids.to(device)\n",
        "            item_ids = item_ids.to(device)\n",
        "            lang_ids = lang_ids.to(device)\n",
        "            numerics = numerics.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits, _, _ = model(user_ids, item_ids, lang_ids, numerics)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "\n",
        "            batch_size = labels.size(0)\n",
        "            total_loss += loss.item() * batch_size\n",
        "            total_examples += batch_size\n",
        "\n",
        "            all_labels.append(labels.detach().cpu().numpy())\n",
        "            all_probs.append(probs.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / max(total_examples, 1)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_probs = np.concatenate(all_probs, axis=0)\n",
        "\n",
        "    if SKLEARN_AVAILABLE:\n",
        "        try:\n",
        "            auc = roc_auc_score(all_labels, all_probs)\n",
        "        except ValueError:\n",
        "            auc = float(\"nan\")\n",
        "    else:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    return avg_loss, auc"
      ],
      "metadata": {
        "id": "n9ErgLejGBNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 6. ROBUST RANKING METRICS (Recall@K, NDCG@K)\n",
        "########################################\n",
        "\n",
        "def compute_dcg(relevances: np.ndarray, k: int) -> float:\n",
        "    \"\"\"\n",
        "    relevances: binary or graded relevance scores sorted in ranking order.\n",
        "    \"\"\"\n",
        "    rel = relevances[:k]\n",
        "    if len(rel) == 0:\n",
        "        return 0.0\n",
        "    discounts = np.log2(np.arange(2, len(rel) + 2))\n",
        "    return float(np.sum((2**rel - 1) / discounts))\n",
        "\n",
        "\n",
        "def evaluate_ranking_with_neg_sampling(\n",
        "    model: TwoTowerRecSys,\n",
        "    test_df: pd.DataFrame,\n",
        "    train_pool: pd.DataFrame,\n",
        "    item_table: pd.DataFrame,\n",
        "    user2idx: Dict[int, int],\n",
        "    item2idx: Dict[int, int],\n",
        "    lang2idx: Dict[int, int],\n",
        "    device: str,\n",
        "    k_list=None,\n",
        "    num_negatives: int = 100,\n",
        "):\n",
        "    \"\"\"\n",
        "    More robust ranking evaluation:\n",
        "    For each user in test_df:\n",
        "      - Positives = repos in test_df with target=1.\n",
        "      - Negatives = sample 'num_negatives' items from full catalog that the user\n",
        "        has never positively interacted with (train_pool + test_df).\n",
        "      - Rank positive + sampled negatives with the model.\n",
        "      - Compute Recall@K and NDCG@K for each K.\n",
        "\n",
        "    Returns: dict like\n",
        "      {\n",
        "        \"Recall@5\": value,\n",
        "        \"Recall@10\": value,\n",
        "        \"NDCG@5\": value,\n",
        "        \"NDCG@10\": value,\n",
        "        \"num_users\": n_users_evaluated\n",
        "      }\n",
        "    \"\"\"\n",
        "    if k_list is None:\n",
        "        k_list = [5, 10]\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Pre-build mapping from project_id -> row index in item_table\n",
        "    item_row_map = {pid: idx for idx, pid in enumerate(item_table[ITEM_ID_COL].values)}\n",
        "    all_item_ids = item_table[ITEM_ID_COL].values\n",
        "\n",
        "    # For each user: set of items they have positively interacted with anywhere\n",
        "    user_pos_all = (\n",
        "        pd.concat([train_pool, test_df], ignore_index=True)\n",
        "        .query(f\"{TARGET_COL} == 1\")\n",
        "        .groupby(USER_ID_COL)[ITEM_ID_COL]\n",
        "        .agg(lambda x: set(x.values))\n",
        "    )\n",
        "\n",
        "    # Positives only from test\n",
        "    user_pos_test = (\n",
        "        test_df.query(f\"{TARGET_COL} == 1\")\n",
        "        .groupby(USER_ID_COL)[ITEM_ID_COL]\n",
        "        .agg(lambda x: list(set(x.values)))\n",
        "    )\n",
        "\n",
        "    # Users we can evaluate on (have positives in test and exist in mapping)\n",
        "    eval_users = [\n",
        "        uid for uid in user_pos_test.index\n",
        "        if (uid in user2idx) and (uid in user_pos_all.index)\n",
        "    ]\n",
        "\n",
        "    recalls = {k: [] for k in k_list}\n",
        "    ndcgs = {k: [] for k in k_list}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for uid in eval_users:\n",
        "            pos_items = user_pos_test[uid]\n",
        "            if len(pos_items) == 0:\n",
        "                continue\n",
        "\n",
        "            all_pos = user_pos_all[uid]\n",
        "            seen_items = set(all_pos)\n",
        "\n",
        "            # Candidate negative pool = all items - seen positives\n",
        "            candidate_neg = [pid for pid in all_item_ids if pid not in seen_items]\n",
        "            if len(candidate_neg) == 0:\n",
        "                continue\n",
        "\n",
        "            # Sample negatives\n",
        "            num_neg = min(num_negatives, len(candidate_neg))\n",
        "            neg_items = random.sample(candidate_neg, num_neg)\n",
        "\n",
        "            # Build candidate list: positives + negatives\n",
        "            cand_items = list(pos_items) + neg_items\n",
        "            cand_labels = np.array(\n",
        "                [1] * len(pos_items) + [0] * len(neg_items),\n",
        "                dtype=np.int32,\n",
        "            )\n",
        "\n",
        "            # Map to item_table rows\n",
        "            cand_indices = [item_row_map[pid] for pid in cand_items]\n",
        "            cand_rows = item_table.iloc[cand_indices]\n",
        "\n",
        "            item_ids_raw = cand_rows[ITEM_ID_COL].values\n",
        "            lang_raw = cand_rows[\"language_code\"].values\n",
        "\n",
        "            item_indices = torch.tensor(\n",
        "                [item2idx[i] for i in item_ids_raw],\n",
        "                dtype=torch.long,\n",
        "                device=device,\n",
        "            )\n",
        "            lang_indices = torch.tensor(\n",
        "                [lang2idx[l] for l in lang_raw],\n",
        "                dtype=torch.long,\n",
        "                device=device,\n",
        "            )\n",
        "            base_numeric = cand_rows[NUMERIC_REPO_COLS].astype(float).values\n",
        "            has_readme = cand_rows[\"has_readme\"].astype(float).values.reshape(-1, 1)\n",
        "            numerics = np.concatenate([base_numeric, has_readme], axis=1)\n",
        "            numerics_tensor = torch.tensor(\n",
        "                numerics,\n",
        "                dtype=torch.float32,\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "            u_idx = user2idx[uid]\n",
        "            user_tensor = torch.tensor([u_idx], dtype=torch.long, device=device)\n",
        "            user_batch = user_tensor.expand(len(cand_items))\n",
        "\n",
        "            logits, _, _ = model(user_batch, item_indices, lang_indices, numerics_tensor)\n",
        "            scores = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "\n",
        "            # Sort candidates by score\n",
        "            ranking = np.argsort(-scores)  # descending\n",
        "            sorted_labels = cand_labels[ranking]\n",
        "\n",
        "            num_pos = float(len(pos_items))\n",
        "\n",
        "            for k in k_list:\n",
        "                k_eff = min(k, len(sorted_labels))\n",
        "                topk_labels = sorted_labels[:k_eff]\n",
        "\n",
        "                # Recall@K: how many positives in top-K divided by total positives\n",
        "                recall_k = float(topk_labels.sum() / num_pos)\n",
        "                recalls[k].append(recall_k)\n",
        "\n",
        "                # NDCG@K\n",
        "                dcg_k = compute_dcg(sorted_labels, k_eff)\n",
        "                # Ideal DCG: all positives ranked at top\n",
        "                ideal_labels = np.sort(cand_labels)[::-1]\n",
        "                idcg_k = compute_dcg(ideal_labels, k_eff)\n",
        "                ndcg_k = dcg_k / idcg_k if idcg_k > 0 else 0.0\n",
        "                ndcgs[k].append(ndcg_k)\n",
        "\n",
        "    metrics = {}\n",
        "    n_users = len(eval_users)\n",
        "    metrics[\"num_users\"] = n_users\n",
        "    for k in k_list:\n",
        "        if len(recalls[k]) > 0:\n",
        "            metrics[f\"Recall@{k}\"] = float(np.mean(recalls[k]))\n",
        "        else:\n",
        "            metrics[f\"Recall@{k}\"] = float(\"nan\")\n",
        "        if len(ndcgs[k]) > 0:\n",
        "            metrics[f\"NDCG@{k}\"] = float(np.mean(ndcgs[k]))\n",
        "        else:\n",
        "            metrics[f\"NDCG@{k}\"] = float(\"nan\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "2Jf5Krd-PUS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# 7. MAIN SCRIPT\n",
        "########################################\n",
        "\n",
        "def main():\n",
        "    cfg = Config()\n",
        "    set_seed(cfg.seed)\n",
        "\n",
        "    print(\"Loading raw data...\")\n",
        "    train_bal, train_neg, test_bal, test_neg = load_raw_data(cfg)\n",
        "\n",
        "    train_pool = pd.concat([train_bal, train_neg], ignore_index=True)\n",
        "    test_df = pd.concat([test_bal, test_neg], ignore_index=True)\n",
        "\n",
        "    print(f\"Train pool size: {len(train_pool)}, Test size: {len(test_df)}\")\n",
        "\n",
        "    print(\"Splitting train pool into train and validation by user...\")\n",
        "    train_df_all, val_df = split_train_val_by_user(\n",
        "        train_pool,\n",
        "        train_user_fraction=cfg.train_user_fraction,\n",
        "        seed=cfg.seed,\n",
        "    )\n",
        "\n",
        "    # IMPORTANT for In-Batch Negatives:\n",
        "    # We only train on POSITIVE interactions. The \"negatives\" are implicitly\n",
        "    # the other items in the batch.\n",
        "    print(\"Filtering training set to keep only POSITIVES (target=1)...\")\n",
        "    train_df = train_df_all[train_df_all[TARGET_COL] == 1].reset_index(drop=True)\n",
        "\n",
        "    # We keep val_df as is (mixed pos/neg) so we can still calculate pointwise AUC\n",
        "    # as a sanity check.\n",
        "\n",
        "    print(f\"Train size (pos only): {len(train_df)}, Val size (mixed): {len(val_df)}\")\n",
        "\n",
        "    print(\"Building ID mappings (users/items/langs)...\")\n",
        "    user2idx, item2idx, lang2idx, idx2user, idx2item, idx2lang = build_id_mappings(\n",
        "        train_pool, test_df\n",
        "    )\n",
        "    num_users = len(user2idx)\n",
        "    num_items = len(item2idx)\n",
        "    num_langs = len(lang2idx)\n",
        "\n",
        "    print(f\"Num users: {num_users}, num items: {num_items}, num langs: {num_langs}\")\n",
        "\n",
        "    num_numeric_feats = len(NUMERIC_REPO_COLS) + 1\n",
        "\n",
        "    # Load item text embeddings (can be None if disabled)\n",
        "    item_text_emb_matrix, text_dim = load_item_text_embeddings(cfg, item2idx)\n",
        "    if not cfg.use_readme_text:\n",
        "        text_dim = 0  # make sure model knows there is no text branch\n",
        "\n",
        "    if cfg.debug_small_train:\n",
        "        # Overfit on a tiny subset of positives\n",
        "        train_df = train_df.sample(\n",
        "            n=min(cfg.debug_small_train_size, len(train_df)),\n",
        "            random_state=cfg.seed,\n",
        "        ).reset_index(drop=True)\n",
        "        print(f\"[DEBUG] Using small training subset: {len(train_df)} rows (positives only)\")\n",
        "\n",
        "    # Load the set of project_ids that have README embeddings\n",
        "    readme_project_ids = get_readme_project_ids(cfg)\n",
        "\n",
        "    def add_has_readme_flag(df: pd.DataFrame, readme_ids: set[int]) -> pd.DataFrame:\n",
        "        # boolean mask -> float32 (0.0 or 1.0)\n",
        "        df[\"has_readme\"] = df[ITEM_ID_COL].astype(\"int64\").isin(readme_ids).astype(\"float32\")\n",
        "        return df\n",
        "\n",
        "    train_pool = add_has_readme_flag(train_pool, readme_project_ids)\n",
        "    train_df   = add_has_readme_flag(train_df,   readme_project_ids)\n",
        "    val_df     = add_has_readme_flag(val_df,     readme_project_ids)\n",
        "    test_df    = add_has_readme_flag(test_df,    readme_project_ids)\n",
        "\n",
        "    print(\"Fitting numeric scalers on training pool...\")\n",
        "    means, stds = compute_numeric_scalers(train_pool)\n",
        "\n",
        "    print(\"Normalizing numeric features...\")\n",
        "    train_df_norm = normalize_numeric_features(train_df, means, stds)\n",
        "    val_df_norm = normalize_numeric_features(val_df, means, stds)\n",
        "    test_df_norm = normalize_numeric_features(test_df, means, stds)\n",
        "\n",
        "    print(\"Creating dataloaders for train and validation...\")\n",
        "    train_loader, val_loader = make_dataloaders(\n",
        "        cfg, train_df_norm, val_df_norm, user2idx, item2idx, lang2idx\n",
        "    )\n",
        "\n",
        "    print(\"Initializing model...\")\n",
        "    model = TwoTowerRecSys(\n",
        "    num_users=num_users,\n",
        "    num_items=num_items,\n",
        "    num_langs=num_langs,\n",
        "    num_numeric_feats=num_numeric_feats,\n",
        "    text_dim=text_dim,\n",
        "    item_text_emb_matrix=item_text_emb_matrix,\n",
        "    cfg=cfg,\n",
        "    ).to(cfg.device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=cfg.lr,\n",
        "        weight_decay=cfg.weight_decay,\n",
        "    )\n",
        "\n",
        "    best_auc = -1.0\n",
        "\n",
        "    for epoch in range(1, cfg.num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{cfg.num_epochs}\")\n",
        "\n",
        "        # Pass temperature to train function\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, cfg.device, cfg.temperature)\n",
        "        print(f\"Train loss (CrossEntropy): {train_loss:.4f}\")\n",
        "\n",
        "        val_loss, val_auc = evaluate_pointwise(model, val_loader, cfg.device)\n",
        "        print(f\"Val loss (Pointwise): {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "        # Note: best model selection is still based on AUC here, which is fine,\n",
        "        # but ideally we would track Recall@K on validation.\n",
        "        if SKLEARN_AVAILABLE and not math.isnan(val_auc) and val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            torch.save(model.state_dict(), \"best_twotower_model.pt\")\n",
        "            print(\"Saved new best model based on validation AUC.\")\n",
        "\n",
        "    # Load best model (if saved)\n",
        "    if os.path.exists(\"best_twotower_model.pt\"):\n",
        "        model.load_state_dict(torch.load(\"best_twotower_model.pt\", map_location=cfg.device))\n",
        "        print(\"Loaded best model from checkpoint.\")\n",
        "\n",
        "    # Final pointwise evaluation on test set (AUC / loss)\n",
        "    print(\"\\nCreating test DataLoader for pointwise metrics...\")\n",
        "    test_dataset = TwoTowerDataset(test_df_norm, user2idx, item2idx, lang2idx)\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    test_loss, test_auc = evaluate_pointwise(model, test_loader, cfg.device)\n",
        "    print(f\"Test loss: {test_loss:.4f}, Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "    # Robust ranking evaluation with negative sampling (Recall@K, NDCG@K)\n",
        "    print(\"\\nBuilding normalized item feature table for ranking evaluation...\")\n",
        "    item_table_norm = build_item_feature_table_norm(train_pool, test_df, means, stds)\n",
        "\n",
        "    print(\"Evaluating ranking metrics with negative sampling...\")\n",
        "    rank_metrics = evaluate_ranking_with_neg_sampling(\n",
        "        model=model,\n",
        "        test_df=test_df,\n",
        "        train_pool=train_pool,\n",
        "        item_table=item_table_norm,\n",
        "        user2idx=user2idx,\n",
        "        item2idx=item2idx,\n",
        "        lang2idx=lang2idx,\n",
        "        device=cfg.device,\n",
        "        k_list=cfg.eval_k_list,\n",
        "        num_negatives=cfg.eval_num_negatives,\n",
        "    )\n",
        "\n",
        "    print(f\"Ranking evaluation over {rank_metrics['num_users']} users:\")\n",
        "    for k in cfg.eval_k_list:\n",
        "        print(\n",
        "            f\"  Recall@{k}: {rank_metrics.get(f'Recall@{k}', float('nan')):.4f}, \"\n",
        "            f\"NDCG@{k}: {rank_metrics.get(f'NDCG@{k}', float('nan')):.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWcQjRk8GEjv",
        "outputId": "0a157247-9302-435c-a6a3-4f3888bf12fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw data...\n",
            "Train pool size: 567339, Test size: 141832\n",
            "Splitting train pool into train and validation by user...\n",
            "Filtering training set to keep only POSITIVES (target=1)...\n",
            "Train size (pos only): 293817, Val size (mixed): 113522\n",
            "Building ID mappings (users/items/langs)...\n",
            "Num users: 10000, num items: 365628, num langs: 119\n",
            "[Text] Loaded README embeddings for 9336 / 365628 items (2.55%).\n",
            "Fitting numeric scalers on training pool...\n",
            "Normalizing numeric features...\n",
            "Creating dataloaders for train and validation...\n",
            "Initializing model...\n",
            "\n",
            "Epoch 1/10\n",
            "Train loss (CrossEntropy): 8.3296\n",
            "Val loss (Pointwise): 0.6902, Val AUC: 0.5015\n",
            "Saved new best model based on validation AUC.\n",
            "\n",
            "Epoch 2/10\n",
            "Train loss (CrossEntropy): 8.2291\n",
            "Val loss (Pointwise): 0.6783, Val AUC: 0.4907\n",
            "\n",
            "Epoch 3/10\n",
            "Train loss (CrossEntropy): 7.5998\n",
            "Val loss (Pointwise): 0.6790, Val AUC: 0.4950\n",
            "\n",
            "Epoch 4/10\n",
            "Train loss (CrossEntropy): 7.0420\n",
            "Val loss (Pointwise): 0.6903, Val AUC: 0.4847\n",
            "\n",
            "Epoch 5/10\n",
            "Train loss (CrossEntropy): 6.6419\n",
            "Val loss (Pointwise): 0.6977, Val AUC: 0.4796\n",
            "\n",
            "Epoch 6/10\n",
            "Train loss (CrossEntropy): 6.3489\n",
            "Val loss (Pointwise): 0.7021, Val AUC: 0.4840\n",
            "\n",
            "Epoch 7/10\n",
            "Train loss (CrossEntropy): 6.0986\n",
            "Val loss (Pointwise): 0.7076, Val AUC: 0.4839\n",
            "\n",
            "Epoch 8/10\n",
            "Train loss (CrossEntropy): 5.8778\n",
            "Val loss (Pointwise): 0.7102, Val AUC: 0.4877\n",
            "\n",
            "Epoch 9/10\n",
            "Train loss (CrossEntropy): 5.7008\n",
            "Val loss (Pointwise): 0.7124, Val AUC: 0.4863\n",
            "\n",
            "Epoch 10/10\n",
            "Train loss (CrossEntropy): 5.5396\n",
            "Val loss (Pointwise): 0.7131, Val AUC: 0.4900\n",
            "Loaded best model from checkpoint.\n",
            "\n",
            "Creating test DataLoader for pointwise metrics...\n",
            "Test loss: 0.6901, Test AUC: 0.5018\n",
            "\n",
            "Building normalized item feature table for ranking evaluation...\n",
            "Evaluating ranking metrics with negative sampling...\n",
            "Ranking evaluation over 10000 users:\n",
            "  Recall@5: 0.0537, NDCG@5: 0.0873\n",
            "  Recall@10: 0.1061, NDCG@10: 0.0994\n"
          ]
        }
      ]
    }
  ]
}